{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter Grader\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-x](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "#### NAME:\n",
    "\n",
    "#### STUDENT ID:\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  HW1: Bootcamp Review \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to load the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this homework is based on https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings. **Please read the Kaggle page for the complete description of the dataset.** We've replaced the column name \"Platform\" with \"Console\" to avoid a conflict due to dummy variables generations (see 3.c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the dataset with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No need for modification, just run this cell\n",
    "df = pd.read_csv(\"HW1_dataset.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Examining Data Integrity and Data Cleaning\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a Data Type Conversion\n",
    "\n",
    "<br>\n",
    "\n",
    "First, we examine the data type of each column. Are there any columns with unexpected data types? (You don't need to write any answers.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No need for modification, just run this cell\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.a.1)** According to the content, \"User_Score\" would be more useful if represented as \"float64\", but its type is \"object\" instead. Try to manually fix this by \n",
    "- Coercing the data type of the \"User_Score\" column to float64.\n",
    "- If there are any entries that can not be converted to numerics (e.g. you might have noticed some entries being \"tbd\", which is why pandas could not convert them into numerics.) interpret them as missing values. \n",
    "\n",
    "**Hint: Select the right option for the \"errors\" argument when converting the values [to numeric](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html#pandas.to_numeric).**\n",
    "\n",
    "*Remeber to store the converted column back to ```df[\"User_Score\"]```.*\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a1\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.b Missing Values and Duplicated Entries\n",
    "\n",
    "<br>\n",
    "\n",
    "**1.b.1)** How many [missing values](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isna.html) are there for each column? Store the answer in a pandas Series (name the variable ```s_nacount```) that is \n",
    "- indexed by the column name of the dataset, and\n",
    "- the corresponding value field stores NaN counts\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b1\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "s_nacount = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.b.2)** Drop all the rows from df that contain missing values. After the operation, df should contain the same column as before but has no NaN in any entries.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b2\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.b.3)** Find out how many duplicated entries the dataset contains. Store the number of [duplicated entries](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html) in the variable ```num_duplicated```. If there are **any duplicated entries, drop them** from the dataframe df.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b3\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "...\n",
    "print(num_duplicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.c Data Sanity Checks\n",
    "\n",
    "<br>\n",
    "\n",
    "**1.c.1)** It is important to check if there are any internal inconsistencies within the dataset. One natural question to ask would be: are \"Global_Sales\" consistent with the regional sales? That is, are the sums of \"NA_Sales\", \"EU_Sales\", \"JP_Sales\", and \"Other_Sales\" equal to \"Global_Sales\" for all entries? Examine this problem by:\n",
    "\n",
    "1. Creating a new column in df named **\"Total_Sales\"** which contains the summation of the columns \"NA_Sales\", \"EU_Sales\", \"JP_Sales\", and \"Other_Sales\".\n",
    "2. Calculating the [absolute](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.abs.html) **difference between \"Total_Sales\" and \"Global_Sales\"** for each entry, and report the largest value of the absolute difference.\n",
    "\n",
    "Store the maximal deviation in a new variable named ```maxdeviation```.\n",
    "\n",
    "Is maxdeviation 0? If not, what are the possible reasons? Is the dataset still acceptible despite nonzero deviations? (You don't need to write any answers)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1c1\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "...\n",
    "maxdeviation = ...\n",
    "print(\"The max deviation between \\\"Total_Sales\\\" and \\\"Global_Sales\\\" is\", maxdeviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.c.2)** Recall that we have removed all duplicated entries from the dataframe, but we still want to make sure there is no subtle web scraping issues such as misspellings that prevent redundant entries from being removed.\n",
    "\n",
    "Does each entry represent one unique game? This question can be divided into two parts.\n",
    "\n",
    "- How many entries (rows) are there in the dataframe now? Store answer (an integer) in a variable named ```len_total```.\n",
    "- How many distinct game names (in the column **\"Name\"**) are there in the dataset? Store the integer result in a variable named ```len_name_unique```.\n",
    "\n",
    "Each entry represents one unique game if and only if the two numbers are equal.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1c2\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "len_total = ...\n",
    "len_name_unique = ...\n",
    "print (\"The number of non-duplicative entries is\", len_total)\n",
    "print (\"The number of distinct game names is\", len_name_unique)\n",
    "print (\"The two numbers are {0}\".format(\"equal\" if len_total==len_name_unique else \"not equal\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.c.3)** To take a deeper look into the structure of the dataset, \n",
    "1. Create a subset of the DataFrame containing only entries of which the game names appear more than once among all entries.\n",
    "2. [Sort](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) the new DateFrame according to the Name **alphabetically in ascending order**.\n",
    "\n",
    "**Hint:  [pandas.DataFrame.groupby](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) and [pandas.core.groupby.DataFrameGroupBy.filter](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.filter.html) may be useful for the tasks in 1. For concrete illustrations and usages, see [the Data100 Lecture](http://www.ds100.org/fa20/lecture/lec06/).**\n",
    "\n",
    "Store the result into ```df_name_multi_sorted```. This practice is intended to address why there are duplicated game names.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1c3\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "...\n",
    "df_name_multi_sorted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'>Important: Before proceeding to the following sections, please make sure you have passed the tests for problems in 1.b. This will ensure df is ready for the following analyses.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the data\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**2.1)** Observe the histogram of the global sales plotted below and try to reproduce it. Specs of the plot:\n",
    "\n",
    "  >1. plot title: 'Histogram of Global Sales'\n",
    "  >2. x-label: 'Global Sales (in millions of unit)'\n",
    "  >3. y-label: 'Count'\n",
    "  >4. bins=np.arange(0, 10, 0.5)\n",
    "\n",
    "<img src=\"./Histogram of Global Sales.png\">\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q21\n",
    "manual: false\n",
    "points: 0\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The question above does not have any points. Just make sure you know how to plot a histogram using either matplotlib or pandas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**2.2)** What are the average \"Global_Sales\", \"Critic_Score\", and \"User_Score\" for each Genre of games?\n",
    "\n",
    "1. Create a new the DataFrame **indexed by \"Genre\"** containing **columns \"Global_Sales\", \"Critic_Score\", and \"User_Score\"** which specifies the [mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html) over all rows of the corresponding genre in df. \n",
    "2. [Sort](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) the new DateFrame according using the **average Global_Sales in descending order**.\n",
    "\n",
    "**Hint: [pandas.DataFrame.groupby](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) and [pandas.core.groupby.DataFrameGroupBy.agg](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html) or [GroupBy.mean](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.mean.html) may be useful for the tasks in 1.**\n",
    "\n",
    "Store the results in a new DataFrame named ```df_mean_genre```.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q22\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "df_mean_genre = ...\n",
    "df_mean_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q22\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**2.3)** The aggregate global sales corresponding to different consoles are plotted on a horizontal bar chart below. Try to reproduce this plot. Specs of plot:\n",
    "\n",
    "  >1. No title\n",
    "  >2. x-label: 'Aggregate Global Sales (in millions of units)'\n",
    "  >3. y-labels are the sroted Console names based on the aggregate global sales (the largest at top)\n",
    "\n",
    "<img src=\"./Horizontal Bar Chart of Aggregate Global Sales.png\">\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q23\n",
    "manual: false\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The question above does not have any points. Just make sure you know how to plot a bar chart using either matplotlib or pandas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**2.4)** Another great way of visulizing the distributed summaries is through the [pivot table](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html).\n",
    "\n",
    "Create a pivot table from df with the following specs:\n",
    "1. The pivot table is indexed by **\"Console\"**, i.e. each row corresponds to one unique game console (PC, PS2, X360, etc.).\n",
    "2. The pivot table has column names corresponding to each unique **genre** (Action, Adventure, etc.).\n",
    "3. Each cell in the pivot table contains the **sum of the \"Global_Sales\"** over all rows in df with the corresonding console and genre.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q24\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "pivot_GlobalSales = ...\n",
    "pivot_GlobalSales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Regression\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will use linear regression to build a model for **predicting a game's global sales** using different sets of features. \n",
    "\n",
    "*Note: For this part, you are not allowed to use scikit-learn tools.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Using Only One Feature\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**3.a.1)** In linear regression, the goal is to find the \"best\" linear model that fits the data. Assume that we only want to use **\"Critic_Score\"** to predict the global sales. \n",
    "\n",
    "The input `x` (critic scores) and the output `y` (global sales) are given to you below. Follow the simple linear regression procedure in these [slides](https://https://datax.berkeley.edu/wp-content/uploads/2020/09/slides-m100-linear-regression.pdf) and find the slope ```m``` and the intercept ```y_0```.\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a1\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['Critic_Score']\n",
    "y = df['Global_Sales']\n",
    "\n",
    "## your code here \n",
    "y_0 = ...\n",
    "m = ...\n",
    "print(m)\n",
    "print(y_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**3.a.2)** We have trained our model on the entire dataset. As a measure for training error, compute the **mean squared error** and store it in ```train_error_a```. Later, you will compare these errors for different models to see how they perform with respect to each other (on the training dataset).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a2\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "train_error_a = ...\n",
    "print(train_error_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**3.a.3)** Below, we have drawn a scatter plot of the output $y$ vs the input $x$ as well as the linear regression line. Try to reproduce this plot. Specs of the plot:\n",
    "\n",
    "  >1. title: 'Global Sale vs Critic Score'\n",
    "  >2. x-label: 'Critic Score'\n",
    "  >3. y-labels:  'Global Sale'\n",
    "\n",
    "<img src=\"./Global Sale vs Critic Score.png\">\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a3\n",
    "manual: false\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The question above does not have any points. Just make sure you know how to plot a scatter plot using either matplotlib or pandas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b Using More Features \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**3.b.1)** Let's include three more features into our regression model. So we will use **\"Critic_Score\", \"Critic_Count\", \"User_Score\", and \"User_Count\"** to train our model. \n",
    "\n",
    "The input `x` and the output `y` are given to you below. Note that `x` has **5 columns**, the first four columns correspond to the four features we want to use and the last column, a **column of ones**, corresponds to the **intercept or the bias term**. Follow the linear regression procedure for multi-dimensional input in these [slides](https://https://datax.berkeley.edu/wp-content/uploads/2020/09/slides-m100-linear-regression.pdf), find the corresponding weight vector, and store it in ```W_b```. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b1\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['Critic_Score' , 'Critic_Count', 'User_Score', 'User_Count']].values\n",
    "x = np.concatenate((x, np.ones((x.shape[0],1))), axis=1)\n",
    "y = df['Global_Sales'].values\n",
    "\n",
    "## your code here\n",
    "W_b = ...\n",
    "print(W_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**3.b.2)** What we hope is to get a **mean squared error** (on the training data) lower than the one computed in the previous part. Could we get a higher error? Why? (You don't need to write any answers.)\n",
    "\n",
    "Compute the **mean squared error** for this new model and assign it to ```train_error_b```.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b2\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "train_error_b = ...\n",
    "print(train_error_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c Dummy Variables\n",
    "\n",
    "<br>\n",
    "\n",
    "Besides a few continuous features, like **Critic_Score**, we have a few valuable discrete features (e.g. Console, Genre, and Rating) that could be encoded into what is recognized by the linear regression model: **the dummy variables**. \n",
    "\n",
    "A dummy variable (aka an indicator variable) is a numeric variable representing categorical data such as Console or Genre in our dataset. They take only the value 0 or 1 to indicate the absence or presence of a qualitative attribute. \n",
    "\n",
    "To represent a categorical variable that can take $n$ different values, we need to define $n-1$ dummy variables. let's say we are looking at some animal pictures and each picture features either a cat, a dog, or an elephant! We can represent what each picture features by introducing two dummy variables $x_0$ and $x_1$:\n",
    "\n",
    "$x_0=1$ if it's a picutre of a cat; $x_0=0$ otherwise.\n",
    "\n",
    "$x_1=1$ if it's a picutre of a dog; $x_1=0$ otherwise.\n",
    "\n",
    "**Note that we don't need a third dummy variable, because if both $x_0$ and $x_1$ are equal to 0, then we find out that the picture features an elephant.** \n",
    "\n",
    "Apart from being redundant, a third dummy variable creates the [dummy variables trap](https://www.algosome.com/articles/dummy-variable-trap-regression.html#:~:text=The%20Dummy%20Variable%20trap%20is,%2Ffemale%20as%20an%20example). We can generate dummy variables using the function [get_dummes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) in pandas. **Make sure you understand the option \"drop_first=True\".** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a dataframe containing the dummy variables. We name it **\"df_with_dummies\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell without any modifications\n",
    "dummy_Genre = pd.get_dummies(df[\"Genre\"], drop_first=True)\n",
    "dummy_Console = pd.get_dummies(df[\"Console\"],drop_first=True)\n",
    "dummy_Rating = pd.get_dummies(df[\"Rating\"],drop_first=True)\n",
    "df_with_dummies = pd.concat([df,dummy_Genre,dummy_Console,dummy_Rating],axis=1)\n",
    "df_with_dummies.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**3.c.1)** Let's take these dummy variables into account. This way, we would have **37 features** to build our regression model. \n",
    "\n",
    "Again, the input `x` (a matrix with **38 columns**) and the output `y` are given to you below. Build your regression model, and store the optimal weights in ```W_c```.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3c1\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = df_with_dummies[['Critic_Score','Critic_Count', 'User_Score', 'User_Count', \n",
    "                       'Adventure', 'Fighting', 'Misc', 'Platform', 'Puzzle', 'Racing', \n",
    "                       'Role-Playing', 'Shooter', 'Simulation', 'Sports', 'Strategy', \n",
    "                       'DC', 'DS', 'GBA', 'GC', 'PC', 'PS', 'PS2', 'PS3', 'PS4', 'PSP', 'PSV', \n",
    "                       'Wii', 'WiiU', 'X360', 'XB', 'XOne', 'E', 'E10+', 'K-A', 'M', 'RP','T']].values\n",
    "x = np.concatenate((x, np.ones((x.shape[0],1))), axis=1)\n",
    "y = df_with_dummies['Global_Sales'].values\n",
    "\n",
    "## your code here\n",
    "W_c = ...\n",
    "print(W_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**3.c.2)** Again, compute the **mean squared error** and store it in ```train_error_c```.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3c2\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "train_error_c = ...\n",
    "print(train_error_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the **mean squared error** computed for each model, which one of these models would you choose? **Is this the right way to pick a model?** (You do not need to write any answers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.d Testing the Models Against a New Dataset\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training errors could be misleading, that is a very low training error does not necessarily mean that our model performs well. The only thing a low training error suggests is that our model fits the training data quite well ([overfitting](https://en.wikipedia.org/wiki/Overfitting)), but it doesn't tell us anything about our model's performance on a new dataset. Therefore, in order to choose the best model, we need to use a brand new dataset (test dataset).\n",
    "\n",
    "We have scraped the web and prepared a new small dataset consisting of four popular titles for **PS4**, released back in 2018: **Red Dead Redemption 2, Marvel's Spider-man, Assassin's Creed Odyssey, and Fifa 18.** \n",
    "\n",
    "Read this small dataset by running the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No need for modification, just run this cell\n",
    "test_dataset_3 = pd.read_csv(\"HW1_test_dataset_3.csv\")\n",
    "test_dataset_3.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**3.d.1)** Let's start with the game **\"Red Dead Redemption 2\"**. What are your models predictions for this game's **global sales**?\n",
    "\n",
    "Create a list ```red_dead_pred``` containing your predictions. \n",
    "\n",
    "(Format: red_dead_pred = [part a prediction, part b prediction, part c prediction])\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3d1\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "red_dead_pred = ...\n",
    "print(red_dead_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3d1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the actual global sales of this game is **13.94 million units**. Which one of the models gave a more accurate prediction? Is that what you expected? (you don't need to write any answers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**3.d.2)** Compute the **mean squared error** for **each of these models** using the **new dataset** and store in the list ```test_error_3```.\n",
    "\n",
    "(Format: test_error_3 = [part a error, part b error, part c error])\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3d2\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "test_error_3 = ...\n",
    "print(test_error_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3d2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the test errors you just computed, **which one of the models would you choose?** Compare the training and test errors. Do you approve using the linear regression model for predicting the global sales? (You don't need to write any answers.)\n",
    "\n",
    "\n",
    "Judging our model based on a **small test dataset** is not a good approach. Remember, we trained our models using **6825 data points**, but we only had **4 data points** to test the models. Furthermore, in many real-world problems, we may not be able to obtain a brand new dataset. You will see in section 4 that how we could resolve these issues by spliting the initial dataset into training and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Analysis with scikit-learn\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will work with `scikit-learn`, an industry standard package for machine learning applications, to build your linear regression model.\n",
    "\n",
    "Run the following cell to load the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the required modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to regress global sales on the 37 features we used in part 3.c. The input matrix `x` (consisting of **37 columns**) and the output `y` (global sales) are given to you below. \n",
    "\n",
    ">**Note:** In part 3.c, the input matrix `x` had 38 columns one of which represented the intercept. When using `scikit-learn`, however, the default setting for the linear regression is to calculate the intercept, so we don't need to add a column of ones manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No need for modification, just run this cell\n",
    "x = df_with_dummies[['Critic_Score','Critic_Count', 'User_Score', 'User_Count', \n",
    "                       'Adventure', 'Fighting', 'Misc', 'Platform', 'Puzzle', 'Racing', \n",
    "                       'Role-Playing', 'Shooter', 'Simulation', 'Sports', 'Strategy', \n",
    "                       'DC', 'DS', 'GBA', 'GC', 'PC', 'PS', 'PS2', 'PS3', 'PS4', 'PSP', 'PSV', \n",
    "                       'Wii', 'WiiU', 'X360', 'XB', 'XOne', 'E', 'E10+', 'K-A', 'M', 'RP','T']]\n",
    "\n",
    "y = df_with_dummies['Global_Sales'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**4.1)** Considering the fact that obtaining a new dataset may not be a feasible option in many real-world problems, in practice, we usually split the original data set into **\"training\" and \"test\" datasets**. We use the \"training\" set to build our model and the \"test\" set to check the model's performance. (In the next homework, you will see that we actually split the original dataset into three sets: \"training\", \"validation\", \"test\".) You can read more about why we need to do train-test spliting [here](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6).\n",
    "\n",
    "Use scikit-learn function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the dataset into \"training\" and \"test\" sets, and store the result in `x_train`, `x_test`, `y_train`, and `y_test`. \n",
    "\n",
    ">**Setting:** Use 10% of the dataset for the test set and let `random_state=2021`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q41\n",
    "manual: false\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q41\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**4.2)** Use scikit-learn function [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linear%20regression#sklearn.linear_model.LinearRegression) to build your model using your training set. **Store your model in `model`.**\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q42\n",
    "manual: false\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have built your model, you can take a look at the model's coefficients, which are given in the attributes **intercept** and **coef**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just run this cell\n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the above numbers compare to the ones you got in part 3.c? **Why are they different?** (You don't need to write any answers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**4.3)** Let's calculate the **mean-squared-error** for the training data using scikit-learn function [mean_squared_error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html?highlight=mean_squared_error#sklearn.metrics.mean_squared_error) and store it in `train_error`.\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q43\n",
    "manual: false\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n",
    "train_error = ...\n",
    "print(train_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q43\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**4.4)** Now, compute the **mean-squared-error** for the test set and store it in `test_error`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q44\n",
    "manual: false\n",
    "points: 3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Your code here\n",
    "test_error = ...\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q44\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the errors you got in the two previous parts with the ones you got in part c. Can you justify why they are not the same? (You don't need to write any answers.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End\n",
    "\n",
    "<br>\n",
    "\n",
    "Congratulations on finishing the homework! Remember to select Kernel/Restart & Run All before submitting your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to create a pdf for your reference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
