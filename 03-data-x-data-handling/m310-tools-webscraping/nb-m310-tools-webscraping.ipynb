{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YbxNTLxUgzl",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Data-X Spring 2020\n",
    "\n",
    "## Notebook: Web Scraping & Web Crawling\n",
    "\n",
    "- How to obtain data the fun way and caveats to take care of when doing so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUpl1ChDUgzm"
   },
   "source": [
    "**Author List**: \n",
    "- Alexander Fred-Ojala\n",
    "- Ishaan Malhi\n",
    "- Sudarshan Gopalakrishnan\n",
    "\n",
    "**Original Sources**: \n",
    "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "- https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "\n",
    "**License**: Feel free to do whatever you want to with this code\n",
    "\n",
    "**Compatibility:** Python >= 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vU34GNZwUgzn"
   },
   "source": [
    "## What we are learning today\n",
    "\n",
    "- Data is seldom all clean and readily available\n",
    "\n",
    "- Often times you might need to gather data from websites because of any or all of the following:\n",
    "\n",
    " - You need to update your data regularly (either as a live stream or in batch, i.e every hour, day etc)\n",
    " - Data isn't readily available, you need to collect data for your tasks\n",
    " - Because it's fun and why not to do it when you can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bHsnPQoLUgzo"
   },
   "source": [
    "### Data Gathering\n",
    "\n",
    " - Web scraping is an important arsenal to have in your toolkit if you want to gather different types of data.\n",
    " - Usually it's a part of the \"Data Gathering\" stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZJvoQmiUUgzp"
   },
   "source": [
    "- Once you gather your data, you clean, prepare/preprocess and use it in your tasks (prediction, analysis etc)\n",
    "\n",
    "<center><img src=\"https://media.giphy.com/media/mG1MxDDEMSAVkF7da3/giphy.gif\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-xzmuOyUgzq"
   },
   "source": [
    "### Popular Web Scraping tools & libraries\n",
    "\n",
    "This notebook mainly goes over how to get data with the Python packages `requests` and  `BeautifulSoup`. However, there are many other Python packages that can be used for scraping.\n",
    "\n",
    "Two very popular and widely used are:\n",
    "\n",
    "* **[Selenium:](http://selenium-python.readthedocs.io/)** Python scraper that can act as a human when visiting websites, almost like a macro. Makes sense of modern Javascript based websites built with React, Angular etc.\n",
    "\n",
    "\n",
    "* **[Scrapy:](https://scrapy.org/)** For automated scripting and has a lot of built in tools for web crawling and scraping that can facilitate the process (e.g. time based, IP rotation etc). Mainly script based scraping for larger projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OfgQJvW8Ugzq"
   },
   "source": [
    "Q: Why do you think a library like `requests` and `BeautifulSoup` can't scrape websites using Frontend Web Apps (like React, Angular, Ember etc)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0zl1uDwUgzs"
   },
   "source": [
    "A: The `requests` library asks for static (html) content. Frontend apps (Angular, React, Ember.js etc) dynamically create/load content on the fly. That itself has led to their popularity since you don't need to render everything from scratch.\n",
    "\n",
    "Fun side note: Frontend web apps also break search engine indexing. If we can't scrape the full site, so can't most search engine bots. A workaround to this is called `Server Side Rendering`. It's an interesting way to \"run\" a frontend web app on the backend and give back static content. Most sites do this nowadays but this is a caveat you should keep in mind when scraping websites.\n",
    "\n",
    "An interesting application in industry is [how Airbnb does this](https://github.com/airbnb/hypernova)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nE_m4OQAUgzs"
   },
   "source": [
    "### API: Application Programming Interfaces\n",
    "\n",
    "Many services offer API's to grab data (Twitter, Wikipedia, Reddit etc.) We have already used an API in the Pandas notebook when we grabbed stock data in CSV format to do analysis. If a good API exists, it is usually the preferred method of obtaining data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cM3MWHAhUgzt"
   },
   "source": [
    "### APIs vs Web Scraping\n",
    "\n",
    "Sometimes APIs don't give us everything we need, OR we need to gather data from websites that don't have an API. In this case, we use Web Scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ggss0zA3Ugzt"
   },
   "source": [
    "# Helpful Web Scraping Cheat Sheet\n",
    "\n",
    "If you want a good documentation of functions in requests and Beautifulsoup (as well as how to save scraped data to an SQLite database), this is a good resource:\n",
    "\n",
    "- https://blog.hartleybrody.com/web-scraping-cheat-sheet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_l7G7pSrUgzu"
   },
   "source": [
    "# Table of Contents\n",
    "(Clickable document links)\n",
    "___\n",
    "\n",
    "### [0: Pre-steup](#sec0)\n",
    "Document setup and Python 2 and Python 3 compability\n",
    "\n",
    "### [1: Simple webscraping intro](#sec1)\n",
    "\n",
    "Simple example of webscraping on a premade HTML template\n",
    "\n",
    "#### Subsection: Scraping Caveats: How to be nice and not make enemies\n",
    "\n",
    "### [2: IMDB top 250 movies w MetaScore](#sec3)\n",
    "\n",
    "Scrape IMDB and compare MetaScore to user reviews.\n",
    "\n",
    "### [3: Scrape Images and Files](#sec4)\n",
    "\n",
    "Scrape a website of Images, PDF's, CSV data or any other file type.\n",
    "\n",
    "## [Breakout Problem: Scrape Weather Data](#secBK)\n",
    "\n",
    "Scrape real time weather data in Berkeley.\n",
    "\n",
    "\n",
    "### [Appendix](#sec5)\n",
    "\n",
    "#### [Scrape Bloomberg sitemap for political news headlines](#sec6)\n",
    "\n",
    "#### [Webcrawl Twitter, recursive URL link fetcher + depth](#sec7)\n",
    "\n",
    "#### [SEO, visualize webite categories as a tree](#sec8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJ-UYYGYUgzu"
   },
   "source": [
    "<a id='sec0'></a>\n",
    "## Pre-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 16
    },
    "colab_type": "code",
    "id": "CId16r7mUgzv",
    "outputId": "037a29f8-47a1-41e6-89ee-8a07db64836e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stretch Jupyter coding blocks to fit screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\")) \n",
    "# if 100% it would fit the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3nVc6Vy7Ugz0"
   },
   "outputs": [],
   "source": [
    "# make it run on py2 and py3\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NHAVel79Ugz3"
   },
   "source": [
    "<a id='sec1'></a>\n",
    "# Webscraping intro\n",
    "\n",
    "In order to scrape content from a website we first need to download the HTML contents of the website. This can be done with the Python library **requests** (with its `.get` method).\n",
    "\n",
    "Then when we want to extract certain information from a website we use the scraping tool **BeautifulSoup4** (import bs4). In order to parse information with beautifulsoup we have to create a soup object from the HTML source code of a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Y7_j3TMDUgz3",
    "outputId": "37bcf1a4-d3fe-4432-dd15-2010e12ebb5b"
   },
   "outputs": [],
   "source": [
    "import requests # The requests library is an HTTP library for getting and posting content etc.\n",
    "import bs4 as bs \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-dgjpOYNUgz6"
   },
   "source": [
    "# Scraping a simple website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XzGBfJSWUgz7"
   },
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'}\n",
    "source = requests.get(\"https://datax.berkeley.edu/public/scrape.php\", headers=headers) \n",
    "# a GET request will download the HTML webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EfR-W7ILUgz_",
    "outputId": "f3437162-63d7-4ebb-d7a2-6ff4cae9b06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "print(source) # If <Response [200]> then \n",
    "# the website has been downloaded succesfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qE8QTSEoUg0F"
   },
   "source": [
    "**Different types of repsonses:**\n",
    "Generally status code starting with 2 indicates success. Status code starting with 4 or 5 indicates error. Frequent appearance of the status codes like 404 (Not Found), 403 (Forbidden), 408 (Request Timeout) might indicate that you got blocked by the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "EcUDu6cLUg0F",
    "outputId": "f8454e9a-056d-401d-d5ab-4fd36f5c8391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n<!DOCTYPE html>\\n<html>\\n<head>\\n\\n<title>Data-X Webscrape Tutorial</title>\\n\\n<style>\\ndiv.container {\\n    width: 100%;\\n    border: 1px solid gray;\\n}\\n\\n.header {\\n    color:#003262;\\n}\\n\\n#second {\\n    font-style: italic;\\n}\\n\\n</style>\\n\\n</head>\\n\\n<body style=\"background-color: #FDB515\">\\n\\n<h1 class=\"header\">Simple Data-X site</h1>\\n\\n\\n<h3 id=\"second\">This site is only live to be scraped.</h3>\\n\\n\\n<div class=\"container\">\\n<p>Some cool text in a container</p>\\n</div>\\n\\n\\n  <h4> Random list </h4>\\n<nav class=\"regular_list\">\\n  <ul>\\n    <li><a href=\"https://en.wikipedia.org/wiki/London\">London</a></li>\\n    <li><a href=\"https://en.wikipedia.org/wiki/Tokyo\">Tokyo</a></li>\\n  </ul>\\n</nav>\\n\\n\\n\\n\\n  <h2>Random London Information within p tags</h2>\\n\\n  <p>London is the capital city of England. It is the most populous city in the  United Kingdom, with a metropolitan area of over 13 million inhabitants.</p>\\n  <p>Standing on the River Thames, London has been a major settlement for two millennia, its history going back to its founding by the Romans, who named it Londinium.</p>\\n\\n<footer>footer content</footer>\\n\\n</div>\\n\\n</body>\\n</html>'\n"
     ]
    }
   ],
   "source": [
    "print(source.content) \n",
    "# This is the HTML content of the website,\n",
    "# as you can see it's quite hard to decipher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "keza_5gsUg0I",
    "outputId": "bca200d2-04d4-49cf-d461-82aa798eb4b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "print(type(source.content)) # type byte in Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ht8vMWtqUg0Q"
   },
   "outputs": [],
   "source": [
    "# Convert source.content to a beautifulsoup object \n",
    "# beautifulsoup can parse (extract specific information) HTML code\n",
    "\n",
    "soup = bs.BeautifulSoup(source.content, features='html.parser') \n",
    "# we pass in the source content\n",
    "# features specifies what type of code we are parsing, \n",
    "# here 'html.parser' specifies that we want beautiful soup to parse HTML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tqX8pjqIUg0T",
    "outputId": "84bf30f7-41ab-4b29-8cb5-bebfcb522991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "colab_type": "code",
    "id": "huP7fcTcUg0X",
    "outputId": "be2f438b-80a1-415f-a83c-bbc7af299f18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<title>Data-X Webscrape Tutorial</title>\n",
      "<style>\n",
      "div.container {\n",
      "    width: 100%;\n",
      "    border: 1px solid gray;\n",
      "}\n",
      "\n",
      ".header {\n",
      "    color:#003262;\n",
      "}\n",
      "\n",
      "#second {\n",
      "    font-style: italic;\n",
      "}\n",
      "\n",
      "</style>\n",
      "</head>\n",
      "<body style=\"background-color: #FDB515\">\n",
      "<h1 class=\"header\">Simple Data-X site</h1>\n",
      "<h3 id=\"second\">This site is only live to be scraped.</h3>\n",
      "<div class=\"container\">\n",
      "<p>Some cool text in a container</p>\n",
      "</div>\n",
      "<h4> Random list </h4>\n",
      "<nav class=\"regular_list\">\n",
      "<ul>\n",
      "<li><a href=\"https://en.wikipedia.org/wiki/London\">London</a></li>\n",
      "<li><a href=\"https://en.wikipedia.org/wiki/Tokyo\">Tokyo</a></li>\n",
      "</ul>\n",
      "</nav>\n",
      "<h2>Random London Information within p tags</h2>\n",
      "<p>London is the capital city of England. It is the most populous city in the  United Kingdom, with a metropolitan area of over 13 million inhabitants.</p>\n",
      "<p>Standing on the River Thames, London has been a major settlement for two millennia, its history going back to its founding by the Romans, who named it Londinium.</p>\n",
      "<footer>footer content</footer>\n",
      "</body></html>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup) # looks a lot nicer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r7AiyC9hUg0a"
   },
   "source": [
    "Above we printed the HTML code of the website, decoded as a beautiful soup object.\n",
    "\n",
    "### HTML tags\n",
    "`<xxx> </xxx>`: are all the HTML tags, that specifies certain sections, stylings etc of the website, for more info: \n",
    "https://www.w3schools.com/tags/ref_byfunc.asp\n",
    "\n",
    "Full list of HTML tags: https://developer.mozilla.org/en-US/docs/Web/HTML/Element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5Lez9n3Ug0b"
   },
   "source": [
    "### HTML DOM Tree\n",
    "\n",
    "The HTML DOM Tree is a logical tree that contains all the objects in a webpage.\n",
    "\n",
    "Any dynamic execution (Javascript, SVG etc) interacts with the DOM tree.\n",
    "\n",
    "Since HTML content has a hierarchy, a Tree structure appropriately models the relationships between different HTML elements.\n",
    "\n",
    "For more, see the [Mozilla Docs](https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model). There's a great intro to the DOM [here](https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RSMQIVhzUg0b"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "## `class` and `id`:\n",
    "\n",
    "class and id attributes of HTML tags, they are used as hooks to give unique styling to certain elements and an id for sections / parts of the page.\n",
    "\n",
    "- **id:** is a unique tag for a specific element (this often does not change)\n",
    "- **class:** specifies a class of objects. Several elements in the HTML code can have the same class.\n",
    "\n",
    "We can use these attributes of an HTML tag to select elements in BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQCX0AwfUg0b"
   },
   "source": [
    "### Suppose we want to extract content that is shown on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "uGVsBqrJUg0c",
    "outputId": "7bbcd4b4-1059-4fda-9f53-10916778e59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body style=\"background-color: #FDB515\">\n",
      "<h1 class=\"header\">Simple Data-X site</h1>\n",
      "<h3 id=\"second\">This site is only live to be scraped.</h3>\n",
      "<div class=\"container\">\n",
      "<p>Some cool text in a container</p>\n",
      "</div>\n",
      "<h4> Random list </h4>\n",
      "<nav class=\"regular_list\">\n",
      "<ul>\n",
      "<li><a href=\"https://en.wikipedia.org/wiki/London\">London</a></li>\n",
      "<li><a href=\"https://en.wikipedia.org/wiki/Tokyo\">Tokyo</a></li>\n",
      "</ul>\n",
      "</nav>\n",
      "<h2>Random London Information within p tags</h2>\n",
      "<p>London is the capital city of England. It is the most populous city in the  United Kingdom, with a metropolitan area of over 13 million inhabitants.</p>\n",
      "<p>Standing on the River Thames, London has been a major settlement for two millennia, its history going back to its founding by the Romans, who named it Londinium.</p>\n",
      "<footer>footer content</footer>\n",
      "</body>\n"
     ]
    }
   ],
   "source": [
    "# Inside the <body> tag of the website is where all the main content is\n",
    "print(soup.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jPlrhEr8Ug0f",
    "outputId": "5ad03326-35ab-445d-ec97-79a76e03f5c3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Data-X Webscrape Tutorial</title>\n"
     ]
    }
   ],
   "source": [
    "print(soup.title) # Title of the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QkO2WRRqUg0i",
    "outputId": "aa6d2901-49e2-4740-da65-4749b2fe8ef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Data-X Webscrape Tutorial</title>\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('title')) # same as .title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6HmsXO_hUg0m",
    "outputId": "ecd62d17-f35f-4f6b-b88f-acdfd34de6e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Some cool text in a container</p>\n"
     ]
    }
   ],
   "source": [
    "# If we want to extract specific text\n",
    "print(soup.find('p')) # will only return first <p> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gmcCODd3Ug0q",
    "outputId": "3ea00dad-a75e-46f8-9a34-1c52e3c5d564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some cool text in a container\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('p').text) # extracts the string within the <p> tag, strips it of tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "O1oPKTr2Ug0s",
    "outputId": "751bf371-01f2-4dc1-ad49-bf94afbbcf02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>Some cool text in a container</p>, <p>London is the capital city of England. It is the most populous city in the  United Kingdom, with a metropolitan area of over 13 million inhabitants.</p>, <p>Standing on the River Thames, London has been a major settlement for two millennia, its history going back to its founding by the Romans, who named it Londinium.</p>]\n"
     ]
    }
   ],
   "source": [
    "# If we want to extract all <p> tags\n",
    "print(soup.find_all('p')) # returns list of all <p> tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N0bIb0ZZUg0v",
    "outputId": "8c679e13-c9d2-467e-afdb-a45c2152802f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"header\">Simple Data-X site</h1>\n"
     ]
    }
   ],
   "source": [
    "# we can also search for classes within all tags, using class_\n",
    "# note that the \"class_\" property is used to distinguish with Python's builtin `class` keyword/function\n",
    "\n",
    "print(soup.find(class_='header')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AFhwtzTbUg0y",
    "outputId": "c93a1133-20b9-492f-df2a-d266e7e90d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h3 id=\"second\">This site is only live to be scraped.</h3>\n"
     ]
    }
   ],
   "source": [
    "# We can also find tags with a specific id\n",
    "\n",
    "print(soup.find(id='second'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "cY1eZHJYUg00",
    "outputId": "da69852c-39a0-4729-9d93-598d3809f253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<nav class=\"regular_list\">\n",
      "<ul>\n",
      "<li><a href=\"https://en.wikipedia.org/wiki/London\">London</a></li>\n",
      "<li><a href=\"https://en.wikipedia.org/wiki/Tokyo\">Tokyo</a></li>\n",
      "</ul>\n",
      "</nav>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all(class_='regular_list')) # find all returns list, \n",
    "# even if there is only one object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "6LexZHPkUg03",
    "outputId": "fc45021d-4253-4eda-969a-050199754cfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some cool text in a container\n",
      "London is the capital city of England. It is the most populous city in the  United Kingdom, with a metropolitan area of over 13 million inhabitants.\n",
      "Standing on the River Thames, London has been a major settlement for two millennia, its history going back to its founding by the Romans, who named it Londinium.\n"
     ]
    }
   ],
   "source": [
    "for p in soup.find_all('p'): # print all text paragraphs on the webpage\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "GgiiTphgUg05",
    "outputId": "cc908aec-d4b9-4fa6-e45c-a71a18473aec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"https://en.wikipedia.org/wiki/London\">London</a>\n",
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "# Extract links / urls\n",
    "# Links in html is usually coded as <a href=\"url\">\n",
    "# where the link is url\n",
    "\n",
    "print(soup.a)\n",
    "print(type(soup.a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Nn5nqwSDUg07",
    "outputId": "d91935dc-dbf0-4225-b3af-aae4c9815749"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/London'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a.get('href') \n",
    "# to get the link from href attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Gy62WwcyUg0_",
    "outputId": "971a7320-aebb-4425-e589-715862b136fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://en.wikipedia.org/wiki/London\">London</a>,\n",
       " <a href=\"https://en.wikipedia.org/wiki/Tokyo\">Tokyo</a>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = soup.find_all('a')\n",
    "\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "IrAJg9o4Ug1B",
    "outputId": "9cae4cda-bc07-4a71-80eb-a8cdcd21596d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info about London:  https://en.wikipedia.org/wiki/London\n",
      "Info about Tokyo:  https://en.wikipedia.org/wiki/Tokyo\n"
     ]
    }
   ],
   "source": [
    "# if we want to list links and their text info\n",
    "\n",
    "links = soup.find_all('a')\n",
    "\n",
    "for l in links:\n",
    "    print(\"Info about {}: \".format(l.text), \\\n",
    "          l.get('href')) \n",
    "# then we have extracted the link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BSVAs1SnUg1E"
   },
   "source": [
    "# Scraping Caveats: How to be nice and not make enemies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vv2ExOfJUg1E"
   },
   "source": [
    "- Webscraping is not always a welcome activity. \n",
    "    As a founder and/or engineer, you don't want to wake up in the middle of the night because your website is down due to scraping!\n",
    "    \n",
    "- When webscraping a website, be mindful and nice and make sure you are not inadvertently sending too many requests to the website, which can lead to a potential problem at the website.\n",
    "\n",
    "- A pretty common reason for websites going temporarily offline is because they get scraped way too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3wlRA0gUg1E"
   },
   "source": [
    "## What do we lookout for when scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MxL9Wp-mUg1F"
   },
   "source": [
    "### Cached content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UvYWEBSlUg1F"
   },
   "source": [
    "- Most website content is usually `cached`. This means the webservers are not serving the content directly, they are cached at a nearby caching server (usually called Point-of-Presence or POPs). Web requests usually hit these servers that are able to serve cached content at a higher frequency.\n",
    "\n",
    "    - You might have heard of services such as AWS Cloudfront or Cloudflare allowing web content to be cached.\n",
    "\n",
    "\n",
    "- That said, some websites might not do this! One way to check is to see the response headers. Let's see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IFUjGnrpUg1G",
    "outputId": "2b32a8b4-2970-448e-a96c-f91988094264"
   },
   "outputs": [],
   "source": [
    "requests.get('https://wikipedia.org').headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZxXBekcKUg1I"
   },
   "source": [
    "The `Cache-Control` header having a non-zero value means the web content is cached.\n",
    "\n",
    "The `X-Cache-Status` equaling `hit-front` or `hit` or similar means you hit a cached content, and the backend server did not directly service request.\n",
    "\n",
    "Yay! This means we can scrape without worrying about taking down Wikipedia.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YZLNHcjkUg1J"
   },
   "source": [
    "\n",
    "\n",
    "# Other useful scraping tips\n",
    "\n",
    "### robots.txt\n",
    "\n",
    "Always check if a website has a `robots.txt` document specifying what parts of the site that you're allowed to scrape (however, the website cannot prevent requests from getting its content, but I'd recommend you all to be nice). It may also contain information about the scraping frequency allowed etc.\n",
    "\n",
    "E.g. \n",
    "- http://www.imdb.com/robots.txt\n",
    "- http://www.nytimes.com/robots.txt\n",
    "\n",
    "### user-agent\n",
    "\n",
    "When you're sending a request to a webpage (no matter if it comes from your computer, iphone, or Python's request package), then you also include a user-agent. This let's the webserver know how to render the contents for you. You can also send user-agent information via a request (to specify who you are for example, or to disguise that you're an automated scraper).\n",
    "\n",
    "Find your machine's / browser's true user agent here: https://www.whoishostingthis.com/tools/user-agent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "A51735tKUg1J",
    "outputId": "4de83160-f6ea-4374-9064-b04836276b20"
   },
   "outputs": [],
   "source": [
    "# user-agent example\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:58.0) Gecko/20100101 Firefox/58.0',\n",
    "    'From': 'data-x@gmail.com' \n",
    "}\n",
    "\n",
    "response = requests.get('http://alex.fo/other/data-x', headers=headers)\n",
    "print(response)\n",
    "print(response.headers) # the response will also have some meta informaiton about the content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QiBHjDHkUg2Y"
   },
   "source": [
    "# Keep a current list IMDB top 250 vs MetaScore\n",
    "\n",
    "Let's say that we want to build an app that can display the most popular movies at the IMDB website.\n",
    "\n",
    "We got to the URL that lists the top 250 movies according to the reviews: http://www.imdb.com/chart/top\n",
    "\n",
    "We see that the entries are stored in a table format, so we try pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Z4VbVAsnUg2Z"
   },
   "outputs": [],
   "source": [
    "df_imdb = pd.read_html('http://www.imdb.com/chart/top',attrs={'class':'chart full-width'})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "aN-7fQyKUg2b",
    "outputId": "02bd68c6-23f1-435a-a14a-7d93d4e86591"
   },
   "outputs": [],
   "source": [
    "df_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bP60R9sXUg2h"
   },
   "outputs": [],
   "source": [
    "df_imdb.drop(df_imdb.columns[[0,3,4]],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "M7tew-QEUg2k",
    "outputId": "1ab416d3-c897-4213-e5b7-7877704f5f20"
   },
   "outputs": [],
   "source": [
    "df_imdb.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_3UQjZexUg2o"
   },
   "outputs": [],
   "source": [
    "# Extract all URLs to find meta score\n",
    "imdb_html = requests.get('http://www.imdb.com/chart/top').content\n",
    "soup = bs.BeautifulSoup(imdb_html, features='html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "w2nu3HR6Ug2r",
    "outputId": "8095ba93-6066-4925-c294-fcaabcee5c41"
   },
   "outputs": [],
   "source": [
    "links = soup.find('table').find_all('a')\n",
    "urls = ['http://www.imdb.com'+l.get('href') for l in links]\n",
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KdGBYvf_Ug2t",
    "outputId": "277cc7ef-ea03-4003-edd2-0a9afe337fc5"
   },
   "outputs": [],
   "source": [
    "urls[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kQCnvOlwUg2v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "meta_scores = np.zeros(250, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "7BiysF2HUg2z",
    "outputId": "cb14e20f-b553-4123-a526-2f67d8e70c1d"
   },
   "outputs": [],
   "source": [
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:58.0) Gecko/20100101 Firefox/58.0',\n",
    "    'From': 'data-x@gmail.com' \n",
    "}\n",
    "\n",
    "for idx,url in enumerate(urls):\n",
    "    print('Getting metscore for movie {}'.format(idx))\n",
    "    film = requests.get(url, headers=headers, timeout=10)\n",
    "    print(film)\n",
    "    soup = bs.BeautifulSoup(film.content, features='html.parser')\n",
    "    info = soup.find(class_='metacriticScore score_favorable titleReviewBarSubItem')\n",
    "    meta_scores[idx] = int(info.find('span').text)\n",
    "    if idx == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zP1yU1cMUg21"
   },
   "outputs": [],
   "source": [
    "df_imdb['meta_scores'] = meta_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "3yN5rbFyUg24",
    "outputId": "971e663d-ec17-47bc-bb5d-d6f6d979e308"
   },
   "outputs": [],
   "source": [
    "df_imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyxQyFIjUg26"
   },
   "source": [
    "<a id='sec4'></a>\n",
    "# Scrape images and other files\n",
    "\n",
    "Let's see how we can automatically find and download files linked at any website.\n",
    "\n",
    "The data you need for your projects might not always be raw data, but in the form of files (images, .txt files etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "uKbl5EUoUg27",
    "outputId": "22b8ecd0-7bfe-4919-dc08-eed01153cfcc"
   },
   "outputs": [],
   "source": [
    "# As we can see there are two images on the data-x.blog/resources\n",
    "# say that we want to download them\n",
    "# Images are displayed with the <img> tag in HTML\n",
    "\n",
    "# open connection and create new soup\n",
    "\n",
    "raw = requests.get('https://data-x.blog/resources/').content\n",
    "soup = bs.BeautifulSoup(raw,features='html.parser')\n",
    "\n",
    "print(soup.find('img')) \n",
    "# as we can see below the image urls \n",
    "# are stored in the src attribute inside the img tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_ztjbl_OUg28",
    "outputId": "083d5df7-7387-4476-c38e-160619654017"
   },
   "outputs": [],
   "source": [
    "# Parse all url to the images\n",
    "img_urls = list()\n",
    "for img in soup.find_all('img'):\n",
    "    img_url = img.get('src') \n",
    "    if '.jpeg' in img_url or '.jpg' in img_url:\n",
    "        print(img_url)\n",
    "        img_urls.append(img_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "djeNGK7mUg2-",
    "outputId": "6b2f1f5e-cd3e-4d36-ba6f-35d1233d8f97"
   },
   "outputs": [],
   "source": [
    "## Let's look at what our current file directory looks like\n",
    "\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "g90o91jVUg3A"
   },
   "outputs": [],
   "source": [
    "# To download and save files with Python we can use \n",
    "# the shutil library which is a file operations library\n",
    "'''\n",
    "The shutil module offers a number of high-level operations on files and \n",
    "collections of files. In particular, functions are provided which support \n",
    "file copying and removal.\n",
    "'''\n",
    "\n",
    "import shutil\n",
    "\n",
    "for idx, img_url in enumerate(img_urls): \n",
    "    #enumarte to create a file integer name for every image\n",
    "    \n",
    "    # make a request to the image URL\n",
    "    img_source = requests.get(img_url, stream=True) \n",
    "    # we set stream = True to download/ \n",
    "    # stream the content of the data\n",
    "    \n",
    "    with open('img'+str(idx)+'.jpg', 'wb') as file: \n",
    "        # open file connection, create file and write to it\n",
    "        shutil.copyfileobj(img_source.raw, file) \n",
    "        # save the raw file object\n",
    "\n",
    "    del img_source # to remove the file from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "a3gpLq2MUg3B",
    "outputId": "335069e0-1b3f-4609-e78b-531346595577"
   },
   "outputs": [],
   "source": [
    "## Let's see if the file has been saved\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2eXM1VA6Ug3F"
   },
   "source": [
    "## Scraping function to download files of any type from a website\n",
    "\n",
    "Below is a function that takes in a website and a specific file type to download X of them from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZOSIo03bUg3F"
   },
   "outputs": [],
   "source": [
    "# Extended scraping function of any file format\n",
    "import os # To interact with operating system and format file name\n",
    "import shutil # To copy file object from python to disk\n",
    "import requests\n",
    "import bs4 as bs\n",
    "\n",
    "def py_file_scraper(url, html_tag='img', source_tag='src', file_type='.jpg',max=-1):\n",
    "    \n",
    "    '''\n",
    "    Function that scrapes a website for certain file formats.\n",
    "    The files will be placed in a folder called \"files\" \n",
    "    in the working directory.\n",
    "    \n",
    "    url = the url we want to scrape from\n",
    "    html_tag = the file tag (usually img for images or \n",
    "    a for file links)\n",
    "    \n",
    "    source_tag = the source tag for the file url \n",
    "    (usually src for images or href for files)\n",
    "    \n",
    "    file_type = .png, .jpg, .pdf, .csv, .xls etc.\n",
    "    \n",
    "    max = integer (max number of files to scrape, \n",
    "    if = -1 it will scrape all files)\n",
    "    '''\n",
    "    \n",
    "    # make a directory called 'files' \n",
    "    # for the files if it does not exist\n",
    "    if not os.path.exists('files/'):\n",
    "        os.makedirs('files/')\n",
    "    print('Loading content from the url...')\n",
    "    source = requests.get(url).content\n",
    "    print('Creating content soup...')\n",
    "    soup = bs.BeautifulSoup(source,'html.parser')\n",
    "    \n",
    "    i=0\n",
    "    print('Finding tag:%s...'%html_tag)\n",
    "    for n, link in enumerate(soup.find_all(html_tag)):\n",
    "        file_url=link.get(source_tag)\n",
    "        print ('\\n',n+1,'. File url',file_url)\n",
    "        \n",
    "        \n",
    "        if 'http' in file_url: # check that it is a valid link\n",
    "            print('It is a valid url..')\n",
    "            \n",
    "            \n",
    "            if file_type in file_url: #only check for specific \n",
    "                # file type\n",
    "                \n",
    "                print('%s FILE TYPE FOUND IN THE URL...'%file_type)\n",
    "                file_name = os.path.splitext(os.path.basename(file_url))[0] + file_type \n",
    "                #extract file name from url\n",
    "\n",
    "                file_source = requests.get(file_url, stream = True)\n",
    "             \n",
    "                # open new stream connection\n",
    "\n",
    "                with open('./files/'+file_name, 'wb') as file: \n",
    "                    # open file connection, create file and \n",
    "                    # write to it\n",
    "                    \n",
    "                    shutil.copyfileobj(file_source.raw, file) \n",
    "                    # save the raw file object\n",
    "                    \n",
    "                    print('DOWNLOADED:',file_name)\n",
    "                    \n",
    "                    i+=1\n",
    "                    \n",
    "                del file_source # delete from memory\n",
    "            else:\n",
    "                print('%s file type NOT found in url:'%file_type)\n",
    "                print('EXCLUDED:',file_url) \n",
    "                # urls not downloaded from\n",
    "                \n",
    "        if i == max:\n",
    "            print('Max reached')\n",
    "            break\n",
    "            \n",
    "\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fNRR7_QUg3H"
   },
   "source": [
    "# Scrape funny cat pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "5oWhsozXUg3H",
    "outputId": "29dc88b7-3e3c-4ce4-fbb9-96604b321c56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "py_file_scraper('https://funcatpictures.com/') \n",
    "# scrape cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "t_ktDHHwUg3J",
    "outputId": "0752ff4c-5311-4290-d42a-2321345d8422"
   },
   "outputs": [],
   "source": [
    "!ls ./files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rlyUE2U1Ug3P"
   },
   "source": [
    "# Scrape real data CSV files from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "XF_1nkeNUg3P",
    "outputId": "f35fecb5-bb8d-4bc2-a565-d6757a634f23",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "py_file_scraper('http://www-eio.upc.edu/~pau/cms/rdata/datasets.html',\n",
    "                html_tag='a', # R data sets\n",
    "                source_tag='href', file_type='.csv',max=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zSRubDrEUg3R"
   },
   "source": [
    "# Extended tip: IP rotation\n",
    "\n",
    "The website might get suspicious if a lot of requests are coming from the same IP address. If you use a shared proxy, VPN or TOR that can help you get around that problem\n",
    "\n",
    "For example:\n",
    "\n",
    "```pyton\n",
    "proxies = {'http' : 'http://10.10.0.0:0000',  \n",
    "          'https': 'http://120.10.0.0:0000'}\n",
    "response = requests.get('https://whateverwebsite.com', proxies=proxies, timeout=5)\n",
    "\n",
    "```\n",
    "\n",
    "Also note the `timeout` argument, this specifies that the request should not be carried out indefinitely (prevents the webserver from detecting scraping activity).\n",
    " \n",
    "\n",
    "By using a shared proxy, the website will see the IP address of the proxy server and not yours. A VPN connects you to another network and the IP address of the VPN provider will be sent to the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnMeFM0XUg3S"
   },
   "source": [
    "## Again, be NICE. Don't use IP rotation unless you know the content is cached and/or you know the website can handle your load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pgxog_7UUg3S"
   },
   "source": [
    "## Websites often rely on IP blocking, rate limiting and other techniques to either:\n",
    "\n",
    "a) Dissuade web scraping\n",
    "\n",
    "b) Block large web scraping requests that turn to Denial of Service (DOS)\n",
    "\n",
    "c) Block scraping on restricted parts of the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9rho3HLUg3S"
   },
   "source": [
    "## IP Blocking\n",
    "- The IP address of your machine(s) is temporarily blocked. (If you are using a VPN, often times this means the *VPNs* IP is blocked, which can inconvenience others using the VPN. Be careful.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z798_m49Ug3T"
   },
   "source": [
    "## Rate Limiting\n",
    "- You might get a 4xx http error if you scrape websites at a higher rate than they can handle. Rate limiting is often multi-tiered, i.e you can be limited per second (concurrent requests), per minute or per hour. Rate limits are sometimes temporary but can be permanent (IP Block) if you routinely hit rate limits.\n",
    "\n",
    "<center><img src=\"https://media.giphy.com/media/8abAbOrQ9rvLG/giphy.gif\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgdY-QZ4Ug3T"
   },
   "source": [
    "## Honeypot Servers\n",
    "- If you are not nice and continue scraping websites at high rates, websites can route your request to special \"Honeypot\" Servers, which essentially redirect your requests to servers that are designed to waste your CPU resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCfQmOgBUg3T"
   },
   "source": [
    "---\n",
    "<a id='secBK'></a>\n",
    "# Breakout problem\n",
    "\n",
    "\n",
    "In this Breakout Problem you should extract live weather data in Berkeley from:\n",
    "\n",
    "[http://forecast.weather.gov/MapClick.php?lat=37.87158815800046&lon=-122.27274583799971](http://forecast.weather.gov/MapClick.php?lat=37.87158815800046&lon=-122.27274583799971)\n",
    "\n",
    "* Task scrape\n",
    "    * period / day (as Tonight, Friday, FridayNight etc.)\n",
    "    * the temperature for the period (as Low, High)\n",
    "    * the long weather description (e.g. Partly cloudy, with a low around 49..)\n",
    "    \n",
    "Store the scraped data strings in a Pandas DataFrame\n",
    "\n",
    "\n",
    "\n",
    "**Hint:** The weather information is found in a div tag with `id='seven-day-forecast'`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YSoP99ZUg3U"
   },
   "source": [
    "\n",
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TaAIzKbUg3U"
   },
   "source": [
    "<a id='sec6'></a>\n",
    "# Scrape Bloomberg sitemap (XML) for current political news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "2O_U1DFDUg3V",
    "outputId": "aa19bef7-6563-4061-ac3e-2aa65ec7467d"
   },
   "outputs": [],
   "source": [
    "# XML documents - site maps, all the urls. just between tags\n",
    "# XML human and machine readable.\n",
    "# Newest links: all the links for FIND SITE MAP!\n",
    "# News websites will have sitemaps for politics, bot constantly\n",
    "# tracking news track the sitemaps\n",
    "\n",
    "# Before scraping a website look at robots.txt file\n",
    "bs.BeautifulSoup(requests.get('https://www.bloomberg.com/robots.txt').content,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Z347Tx8AUg3Y"
   },
   "outputs": [],
   "source": [
    "source = requests.get('https://www.bloomberg.com/feeds/bpol/sitemap_news.xml').content\n",
    "soup = bs.BeautifulSoup(source,'xml') # Note parser 'xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "DfXZ_h7NUg3b",
    "outputId": "9eabacdc-f807-43b1-8c62-83dd971cbb56"
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Lg_jqG9_Ug3f",
    "outputId": "9ff8dc49-9158-4970-97bd-67a221766caa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find political news headlines\n",
    "for news in soup.find_all({'news'}):\n",
    "    print(news.title.text)\n",
    "    print(news.publication_date.text)\n",
    "    #print(news.keywords.text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YdRa7kiiUg3i"
   },
   "source": [
    "<a id='sec7'></a>\n",
    "# Web crawl\n",
    "\n",
    "Web crawling is almost like webscraping, but instead you crawl a specific website (and often its subsites) and extract meta information. It can be seen as simple, recursive scraping. This can be used for web indexing (in order to build a web search engine)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLC3m-yEUg3i"
   },
   "source": [
    "## Web crawl Twitter account\n",
    "**Authors:** Kunal Desai & Alexander Fred Ojala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SxC8fuaeUg3i"
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "B6iz46YLUg3m"
   },
   "outputs": [],
   "source": [
    "# Helper function to maintain the urls and the number of times they appear\n",
    "\n",
    "url_dict = dict()\n",
    "\n",
    "def add_to_dict(url_d, key):\n",
    "    if key in url_d:\n",
    "        url_d[key] = url_d[key] + 1\n",
    "    else:\n",
    "        url_d[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "v9Ahqmg7Ug3p"
   },
   "outputs": [],
   "source": [
    "# Recursive function which extracts links from the given url upto a given 'depth'.\n",
    "\n",
    "def get_urls(url, depth):\n",
    "    if depth == 0:\n",
    "        return\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.has_attr('href') and \"https://\" in link['href']:\n",
    "#             print(link['href'])\n",
    "            add_to_dict(url_dict, link['href'])\n",
    "            get_urls(link['href'], depth - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nbsxCWMgUg3s"
   },
   "outputs": [],
   "source": [
    "# Iterative function which extracts links from the given url upto a given 'depth'.\n",
    "\n",
    "def get_urls_iterative(url, depth):\n",
    "    urls = [url]\n",
    "    for url in urls:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            if link.has_attr('href') and \"https://\" in link['href']:\n",
    "                add_to_dict(url_dict, link['href'])\n",
    "                urls.append(link['href'])\n",
    "        if len(urls) > depth:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MSIwJiEgUg3u"
   },
   "outputs": [],
   "source": [
    "get_urls(\"https://twitter.com/GolfWorld\", 2)\n",
    "for key in url_dict:\n",
    "    print(str(key) + \"  ----   \" + str(url_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVOx8Us5Ug3v"
   },
   "source": [
    "<a id='sec8'></a>\n",
    "# SEO: Visualize sitemap and categories in a website\n",
    "\n",
    "**Source:** https://www.ayima.com/guides/how-to-visualize-an-xml-sitemap-using-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Lh5gUlBzUg3w",
    "outputId": "f35ec054-3c5f-4ae6-bc5c-8f2cd24f65be"
   },
   "outputs": [],
   "source": [
    "# Visualize XML sitemap with categories!\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.sportchek.ca/sitemap.xml'\n",
    "url = 'https://www.bloomberg.com/feeds/bpol/sitemap_index.xml'\n",
    "page = requests.get(url)\n",
    "print('Loaded page with: %s' % page)\n",
    "\n",
    "sitemap_index = BeautifulSoup(page.content, 'html.parser')\n",
    "print('Created %s object' % type(sitemap_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IDloFQhHUg3x",
    "outputId": "b702f400-49ce-492b-fdfa-4362f33bb929",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls = [element.text for element in sitemap_index.findAll('loc')]\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Qc1oudSSUg3y",
    "outputId": "78f89cfd-5572-45bb-caf2-fd37a9fec6af"
   },
   "outputs": [],
   "source": [
    "def extract_links(url):\n",
    "    ''' Open an XML sitemap and find content wrapped in loc tags. '''\n",
    "\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    links = [element.text for element in soup.findAll('loc')]\n",
    "\n",
    "    return links\n",
    "\n",
    "sitemap_urls = []\n",
    "for url in urls:\n",
    "    links = extract_links(url)\n",
    "    sitemap_urls += links\n",
    "\n",
    "print('Found {:,} URLs in the sitemap'.format(len(sitemap_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uV5D5-faUg31"
   },
   "outputs": [],
   "source": [
    "with open('sitemap_urls.dat', 'w') as f:\n",
    "    for url in sitemap_urls:\n",
    "        f.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "cLYtSYlqUg33",
    "outputId": "724a1660-41c2-4b7c-f2dd-b0ce739142ce"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Categorize a list of URLs by site path.\n",
    "The file containing the URLs should exist in the working directory and be\n",
    "named sitemap_urls.dat. It should contain one URL per line.\n",
    "Categorization depth can be specified by executing a call like this in the\n",
    "terminal (where we set the granularity depth level to 5):\n",
    "    python categorize_urls.py --depth 5\n",
    "The same result can be achieved by setting the categorization_depth variable\n",
    "manually at the head of this file and running the script with:\n",
    "    python categorize_urls.py\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "categorization_depth=3\n",
    "\n",
    "\n",
    "\n",
    "# Main script functions\n",
    "\n",
    "\n",
    "def peel_layers(urls, layers=3):\n",
    "    ''' Builds a dataframe containing all unique page identifiers up\n",
    "    to a specified depth and counts the number of sub-pages for each.\n",
    "    Prints results to a CSV file.\n",
    "    urls : list\n",
    "        List of page URLs.\n",
    "    layers : int\n",
    "        Depth of automated URL search. Large values for this parameter\n",
    "        may cause long runtimes depending on the number of URLs.\n",
    "    '''\n",
    "\n",
    "    # Store results in a dataframe\n",
    "    sitemap_layers = pd.DataFrame()\n",
    "\n",
    "    # Get base levels\n",
    "    bases = pd.Series([url.split('//')[-1].split('/')[0] for url in urls])\n",
    "    sitemap_layers[0] = bases\n",
    "\n",
    "    # Get specified number of layers\n",
    "    for layer in range(1, layers+1):\n",
    "\n",
    "        page_layer = []\n",
    "        for url, base in zip(urls, bases):\n",
    "            try:\n",
    "                page_layer.append(url.split(base)[-1].split('/')[layer])\n",
    "            except:\n",
    "                # There is nothing that deep!\n",
    "                page_layer.append('')\n",
    "\n",
    "        sitemap_layers[layer] = page_layer\n",
    "\n",
    "    # Count and drop duplicate rows + sort\n",
    "    sitemap_layers = sitemap_layers.groupby(list(range(0, layers+1)))[0].count()\\\n",
    "                     .rename('counts').reset_index()\\\n",
    "                     .sort_values('counts', ascending=False)\\\n",
    "                     .sort_values(list(range(0, layers)), ascending=True)\\\n",
    "                     .reset_index(drop=True)\n",
    "\n",
    "    # Convert column names to string types and export\n",
    "    sitemap_layers.columns = [str(col) for col in sitemap_layers.columns]\n",
    "    sitemap_layers.to_csv('sitemap_layers.csv', index=False)\n",
    "\n",
    "    # Return the dataframe\n",
    "    return sitemap_layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sitemap_urls = open('sitemap_urls.dat', 'r').read().splitlines()\n",
    "print('Loaded {:,} URLs'.format(len(sitemap_urls)))\n",
    "\n",
    "print('Categorizing up to a depth of %d' % categorization_depth)\n",
    "sitemap_layers = peel_layers(urls=sitemap_urls,\n",
    "                             layers=categorization_depth)\n",
    "print('Printed {:,} rows of data to sitemap_layers.csv'.format(len(sitemap_layers)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Qg4l_ty3Ug35",
    "outputId": "736eedae-b239-4a6f-b30e-d96cf948829e"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Visualize a list of URLs by site path.\n",
    "This script reads in the sitemap_layers.csv file created by the\n",
    "categorize_urls.py script and builds a graph visualization using Graphviz.\n",
    "Graph depth can be specified by executing a call like this in the\n",
    "terminal:\n",
    "    python visualize_urls.py --depth 4 --limit 10 --title \"My Sitemap\" --style \"dark\" --size \"40\"\n",
    "The same result can be achieved by setting the variables manually at the head\n",
    "of this file and running the script with:\n",
    "    python visualize_urls.py\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "# Set global variables\n",
    "\n",
    "graph_depth = 3  # Number of layers deep to plot categorization\n",
    "limit = 3       # Maximum number of nodes for a branch\n",
    "title = ''       # Graph title\n",
    "style = 'light'  # Graph style, can be \"light\" or \"dark\"\n",
    "size = '8,5'     # Size of rendered PDF graph\n",
    "\n",
    "\n",
    "# Import external library dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "\n",
    "\n",
    "\n",
    "# Main script functions\n",
    "\n",
    "def make_sitemap_graph(df, layers=3, limit=50, size='8,5'):\n",
    "    ''' Make a sitemap graph up to a specified layer depth.\n",
    "    sitemap_layers : DataFrame\n",
    "        The dataframe created by the peel_layers function\n",
    "        containing sitemap information.\n",
    "    layers : int\n",
    "        Maximum depth to plot.\n",
    "    limit : int\n",
    "        The maximum number node edge connections. Good to set this\n",
    "        low for visualizing deep into site maps.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Check to make sure we are not trying to plot too many layers\n",
    "    if layers > len(df) - 1:\n",
    "        layers = len(df)-1\n",
    "        print('There are only %d layers available to plot, setting layers=%d'\n",
    "              % (layers, layers))\n",
    "\n",
    "\n",
    "    # Initialize graph\n",
    "    f = graphviz.Digraph('sitemap', filename='sitemap_graph_%d_layer' % layers)\n",
    "    f.body.extend(['rankdir=LR', 'size=\"%s\"' % size])\n",
    "\n",
    "\n",
    "    def add_branch(f, names, vals, limit, connect_to=''):\n",
    "        ''' Adds a set of nodes and edges to nodes on the previous layer. '''\n",
    "\n",
    "        # Get the currently existing node names\n",
    "        node_names = [item.split('\"')[1] for item in f.body if 'label' in item]\n",
    "\n",
    "        # Only add a new branch it it will connect to a previously created node\n",
    "        if connect_to:\n",
    "            if connect_to in node_names:\n",
    "                for name, val in list(zip(names, vals))[:limit]:\n",
    "                    f.node(name='%s-%s' % (connect_to, name), label=name)\n",
    "                    f.edge(connect_to, '%s-%s' % (connect_to, name), label='{:,}'.format(val))\n",
    "\n",
    "\n",
    "    f.attr('node', shape='rectangle') # Plot nodes as rectangles\n",
    "\n",
    "    # Add the first layer of nodes\n",
    "    for name, counts in df.groupby(['0'])['counts'].sum().reset_index()\\\n",
    "                          .sort_values(['counts'], ascending=False).values:\n",
    "        f.node(name=name, label='{} ({:,})'.format(name, counts))\n",
    "\n",
    "    if layers == 0:\n",
    "        return f\n",
    "\n",
    "    f.attr('node', shape='oval') # Plot nodes as ovals\n",
    "    f.graph_attr.update()\n",
    "\n",
    "    # Loop over each layer adding nodes and edges to prior nodes\n",
    "    for i in range(1, layers+1):\n",
    "        cols = [str(i_) for i_ in range(i)]\n",
    "        nodes = df[cols].drop_duplicates().values\n",
    "        for j, k in enumerate(nodes):\n",
    "\n",
    "            # Compute the mask to select correct data\n",
    "            mask = True\n",
    "            for j_, ki in enumerate(k):\n",
    "                mask &= df[str(j_)] == ki\n",
    "\n",
    "            # Select the data then count branch size, sort, and truncate\n",
    "            data = df[mask].groupby([str(i)])['counts'].sum()\\\n",
    "                    .reset_index().sort_values(['counts'], ascending=False)\n",
    "\n",
    "            # Add to the graph\n",
    "            add_branch(f,\n",
    "                       names=data[str(i)].values,\n",
    "                       vals=data['counts'].values,\n",
    "                       limit=limit,\n",
    "                       connect_to='-'.join(['%s']*i) % tuple(k))\n",
    "\n",
    "            print(('Built graph up to node %d / %d in layer %d' % (j, len(nodes), i))\\\n",
    "                    .ljust(50), end='\\r')\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def apply_style(f, style, title=''):\n",
    "    ''' Apply the style and add a title if desired. More styling options are\n",
    "    documented here: http://www.graphviz.org/doc/info/attrs.html#d:style\n",
    "    f : graphviz.dot.Digraph\n",
    "        The graph object as created by graphviz.\n",
    "    style : str\n",
    "        Available styles: 'light', 'dark'\n",
    "    title : str\n",
    "        Optional title placed at the bottom of the graph.\n",
    "    '''\n",
    "\n",
    "    dark_style = {\n",
    "        'graph': {\n",
    "            'label': title,\n",
    "            'bgcolor': '#3a3a3a',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '18',\n",
    "            'fontcolor': 'white',\n",
    "        },\n",
    "        'nodes': {\n",
    "            'style': 'filled',\n",
    "            'color': 'white',\n",
    "            'fillcolor': 'black',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '14',\n",
    "            'fontcolor': 'white',\n",
    "        },\n",
    "        'edges': {\n",
    "            'color': 'white',\n",
    "            'arrowhead': 'open',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '12',\n",
    "            'fontcolor': 'white',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    light_style = {\n",
    "        'graph': {\n",
    "            'label': title,\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '18',\n",
    "            'fontcolor': 'black',\n",
    "        },\n",
    "        'nodes': {\n",
    "            'style': 'filled',\n",
    "            'color': 'black',\n",
    "            'fillcolor': '#dbdddd',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '14',\n",
    "            'fontcolor': 'black',\n",
    "        },\n",
    "        'edges': {\n",
    "            'color': 'black',\n",
    "            'arrowhead': 'open',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '12',\n",
    "            'fontcolor': 'black',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if style == 'light':\n",
    "        apply_style = light_style\n",
    "\n",
    "    elif style == 'dark':\n",
    "        apply_style = dark_style\n",
    "\n",
    "    f.graph_attr = apply_style['graph']\n",
    "    f.node_attr = apply_style['nodes']\n",
    "    f.edge_attr = apply_style['edges']\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read in categorized data\n",
    "sitemap_layers = pd.read_csv('sitemap_layers.csv', dtype=str)\n",
    "# Convert numerical column to integer\n",
    "sitemap_layers.counts = sitemap_layers.counts.apply(int)\n",
    "print('Loaded {:,} rows of categorized data from sitemap_layers.csv'\\\n",
    "        .format(len(sitemap_layers)))\n",
    "\n",
    "print('Building %d layer deep sitemap graph' % graph_depth)\n",
    "f = make_sitemap_graph(sitemap_layers, layers=graph_depth,\n",
    "                       limit=limit, size=size)\n",
    "f = apply_style(f, style=style, title=title)\n",
    "\n",
    "f.render(cleanup=True)\n",
    "print('Exported graph to sitemap_graph_%d_layer.pdf' % graph_depth)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KjC4qxVqUg39"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "r7AiyC9hUg0a",
    "G5Lez9n3Ug0b",
    "N3wlRA0gUg1E",
    "MxL9Wp-mUg1F",
    "YZLNHcjkUg1J",
    "QiBHjDHkUg2Y",
    "tyxQyFIjUg26",
    "2eXM1VA6Ug3F",
    "8fNRR7_QUg3H",
    "rlyUE2U1Ug3P",
    "zSRubDrEUg3R",
    "TnMeFM0XUg3S",
    "Pgxog_7UUg3S",
    "U9rho3HLUg3S",
    "Z798_m49Ug3T",
    "QgdY-QZ4Ug3T",
    "VCfQmOgBUg3T",
    "-YSoP99ZUg3U",
    "0TaAIzKbUg3U",
    "YdRa7kiiUg3i",
    "GLC3m-yEUg3i",
    "mVOx8Us5Ug3v"
   ],
   "name": "Notebook_WebScraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
