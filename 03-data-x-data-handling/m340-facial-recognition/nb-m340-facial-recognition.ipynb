{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# Computer Vision (Face Recognition & EmoPy Python Libraries)\n",
    "## Demo\n",
    "> #### IEOR 135/290, Data-X: Applied Data Ventures\n",
    "> Joshua Rafael Sanchez | UC Berkeley, B.S. IEOR, '20 | joshuarafael@berkeley.edu <br>\n",
    "> Made in collaboration with Ikalaq Sidhu, Arash Nourian, and Elias Castro-Hernandez.\n",
    "\n",
    "### About This Notebook:\n",
    "> This notebook contains the demo of the Computer Vision content in Data-X.  `Face_recognition` and `EmoPy` libraries are used. All questions under this demo are drawn from the concepts mentioned in the course.  \n",
    "\n",
    "> __Instructions:__ \n",
    "> Run the cells below to run the demo.  Install the libraries before running the demo, and make sure that the libraries are up to date.  This notebook was created in Summer 2020. \n",
    "\n",
    "> __Resources Used:__\n",
    "> - The following link goes through the content used for the `face_recognition` library in detail:  https://github.com/ageitgey/face_recognition \n",
    "> - The following link goes through the content used for the `EmoPy` library in detail:  https://github.com/thoughtworksarts/EmoPy \n",
    "\n",
    "> __Copyright:__ \n",
    "> Feel free to use the code as you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='table_of_contents'></a>\n",
    "## Table of Contents\n",
    "\n",
    "### [1.](#part_1)  Face Recognition (`face_recognition`)\n",
    "> Here, we will import libraries first, and then go through different features which are offered in the face_recognition library.  They are divided by what you can do with the library:\n",
    "- [1.0](#part_1.0) Introduction & Setup\n",
    "- [1.1](#part_1.1) Finding faces in pictures (face_location)\n",
    "- [1.2](#part_1.2) Find and manipulate facial features in pictures (face_landmarks)\n",
    "- [1.3](#part_1.3) Recognize faces in images and identify who they are (compare_faces)\n",
    "- [1.4](#part_1.4) Face recognition from webcam (video_capture)\n",
    "\n",
    "### [2.](#part_2) EmoPy (`EmoPy`)\n",
    "> We will import libraries first, and then go through an introductory example in EmoPy Python library.\n",
    "- [2.0](#part_2.0) Introduction & Setup\n",
    "- [2.1](#part_2.1) Generate emotion probabilites for sample image (predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part_1'></a>\n",
    "## 1. Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part_1.0'></a>\n",
    "### 1.0 Introduction & Setup\n",
    "[Back to the table of contents.](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction- About the library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From pypi.org: <br> \"Recognize and manipulate faces from Python or from the command line with\n",
    "the world’s simplest face recognition library.  Built using [dlib](http://dlib.net/)’s state-of-the-art face recognition\n",
    "built with deep learning. The model has an accuracy of 99.38% on the\n",
    "[Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) benchmark.\"\n",
    "\n",
    "> **Note:** dlib is not used in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction- About the Files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following images are used in the notebook (scaled down below). You can access them directly:\n",
    "- `biden.jpg`: Portrait of Joe Biden\n",
    "- `obama.jpg`: Portrait of Barack Obama\n",
    "- `obama2.jpg`: Portrait of Barack Obama (different than previous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"biden.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "<td> <img src=\"obama.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "<td> <img src=\"obama2.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup- Installing libraries (on Mac or Linux):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First, you'll need to make sure you have the following packages installed:\n",
    "- `cmake`: Free and open-source software tool\n",
    "- `dlib`: Modern C++ toolkit containing machine learning algorithms and tools to create complex software\n",
    "\n",
    "> **Tip:** First, make sure you have dlib already installed with Python bindings: [Link](https://gist.github.com/ageitgey/629d75c1baac34dfa5ca2a1928a7aeaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in /Users/Joshua/anaconda3/lib/python3.6/site-packages (3.18.0)\n",
      "Requirement already satisfied: dlib in /Users/Joshua/anaconda3/lib/python3.6/site-packages (19.20.0)\n"
     ]
    }
   ],
   "source": [
    "# You need to install dlib, probably using pip install.\n",
    "# note: installing dlib might take more than a few minutes.\n",
    "!pip install cmake\n",
    "!pip install dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then, install `face_recognition` using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: face_recognition in /Users/Joshua/anaconda3/lib/python3.6/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from face_recognition) (1.14.5)\n",
      "Requirement already satisfied: Pillow in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from face_recognition) (5.0.0)\n",
      "Requirement already satisfied: face-recognition-models>=0.3.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from face_recognition) (0.3.0)\n",
      "Requirement already satisfied: dlib>=19.7 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from face_recognition) (19.20.0)\n",
      "Requirement already satisfied: Click>=6.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from face_recognition) (6.7)\n"
     ]
    }
   ],
   "source": [
    "# This may take a couple minutes.\n",
    "!pip3 install face_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then, install OpenCV. Installing may be different depending on your system.  (This will allow you to use the webcam in `face_recognition`.) <br>\n",
    "> Resource: https://pypi.org/project/opencv-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/Joshua/anaconda3/lib/python3.6/site-packages (4.3.0.36)\r\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from opencv-python) (1.14.5)\r\n"
     ]
    }
   ],
   "source": [
    "# This may take 5-10 minutes, if OpenCV is not installed.\n",
    "!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally, make sure to check the version of `face_recognition`.  We used Version 1.2.3 in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using Version 1.2.3\n"
     ]
    }
   ],
   "source": [
    "print('You are using Version', face_recognition.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part_1.1'></a>\n",
    "### 1.1 Finding faces in pictures (`face_location`)\n",
    "[Back to the table of contents.](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Find all the faces that appear in a picture. Here is sample image input and the output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"example_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: We are using `biden.jpg` for this example.\n",
    "\n",
    "> **Note:** We are importing `face_recognition` and `Image` from `PIL` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found 1 face(s) in this photograph.\n",
      "A face is located at pixel location Top: 241, Left: 419, Bottom: 562, Right: 740\n"
     ]
    }
   ],
   "source": [
    "# Importing PIL.Image and face_recognition\n",
    "from PIL import Image\n",
    "import face_recognition\n",
    "\n",
    "# Load the jpg file into a numpy array\n",
    "image = face_recognition.load_image_file(\"biden.jpg\") # INSERT IMAGE HERE\n",
    "\n",
    "# Find all the faces in the image using the default HOG-based model.\n",
    "# This method is fairly accurate, but not as accurate as the CNN model and not GPU accelerated.\n",
    "# See also: find_faces_in_picture_cnn.py\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "print(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n",
    "\n",
    "for face_location in face_locations:\n",
    "\n",
    "    # Print the location of each face in this image\n",
    "    top, right, bottom, left = face_location\n",
    "    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n",
    "\n",
    "    # You can access the actual face itself like this:\n",
    "    face_image = image[top:bottom, left:right]\n",
    "    pil_image = Image.fromarray(face_image)\n",
    "    pil_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In other words, the top-left corner of the image is located at (241,419), and the bottom-right corner of the image is locatd at (562, 740).\n",
    "\n",
    "> An image like this should appear in a new pop-up:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"example_1_1.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part_1.2'></a>\n",
    "### 1.2 Find and manipulate facial features in pictures (`face_landmarks`)\n",
    "[Back to the table of contents.](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Get the locations and outlines of each person's eyes, nose, mouth and chin. Here is the sample image input and the output we will be using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"example_2.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: We are using `biden.jpg` for this example. <br>\n",
    "> **Note:** We are importing `face_recognition` and `Image` and `ImageDraw` from `PIL` here.\n",
    "> **Tip:** Documentation for `PIL` library differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.0.0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the version of PIL for documentation:\n",
    "import PIL # import library just to check your version.\n",
    "PIL.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found 11 face(s) in this photograph.\n",
      "The chin in this face has the following points: [(1356, 440), (1356, 452), (1357, 463), (1358, 475), (1361, 486), (1368, 495), (1378, 502), (1388, 507), (1399, 509), (1410, 508), (1421, 503), (1431, 497), (1437, 488), (1441, 479), (1442, 469), (1443, 459), (1443, 449)]\n",
      "The left_eyebrow in this face has the following points: [(1370, 430), (1377, 426), (1386, 425), (1394, 427), (1401, 431)]\n",
      "The right_eyebrow in this face has the following points: [(1414, 433), (1421, 432), (1428, 432), (1435, 434), (1439, 440)]\n",
      "The nose_bridge in this face has the following points: [(1407, 440), (1406, 446), (1406, 452), (1405, 458)]\n",
      "The nose_tip in this face has the following points: [(1395, 463), (1399, 465), (1404, 466), (1408, 466), (1412, 465)]\n",
      "The left_eye in this face has the following points: [(1379, 438), (1384, 436), (1389, 437), (1394, 441), (1388, 441), (1383, 440)]\n",
      "The right_eye in this face has the following points: [(1416, 445), (1421, 442), (1426, 442), (1430, 445), (1426, 446), (1421, 445)]\n",
      "The top_lip in this face has the following points: [(1380, 474), (1389, 472), (1397, 471), (1403, 472), (1408, 472), (1416, 474), (1422, 478), (1420, 478), (1408, 476), (1402, 476), (1397, 475), (1382, 475)]\n",
      "The bottom_lip in this face has the following points: [(1422, 478), (1415, 486), (1407, 489), (1401, 489), (1395, 488), (1387, 484), (1380, 474), (1382, 475), (1396, 482), (1402, 483), (1408, 483), (1420, 478)]\n",
      "The chin in this face has the following points: [(818, 428), (819, 441), (821, 454), (824, 466), (830, 477), (839, 487), (849, 495), (859, 502), (870, 503), (879, 500), (886, 490), (893, 482), (899, 473), (903, 463), (905, 452), (905, 442), (906, 432)]\n",
      "The left_eyebrow in this face has the following points: [(835, 427), (841, 423), (849, 421), (857, 422), (864, 426)]\n",
      "The right_eyebrow in this face has the following points: [(879, 426), (885, 424), (892, 423), (898, 424), (902, 428)]\n",
      "The nose_bridge in this face has the following points: [(873, 431), (874, 439), (874, 447), (875, 455)]\n",
      "The nose_tip in this face has the following points: [(864, 459), (868, 460), (873, 462), (878, 460), (881, 458)]\n",
      "The left_eye in this face has the following points: [(845, 432), (850, 430), (855, 430), (859, 433), (855, 433), (850, 433)]\n",
      "The right_eye in this face has the following points: [(881, 433), (886, 430), (890, 430), (894, 433), (890, 433), (886, 433)]\n",
      "The top_lip in this face has the following points: [(848, 469), (857, 468), (867, 468), (872, 468), (877, 467), (883, 467), (888, 468), (886, 469), (877, 470), (872, 471), (867, 470), (849, 470)]\n",
      "The bottom_lip in this face has the following points: [(888, 468), (883, 478), (877, 484), (872, 485), (866, 485), (856, 480), (848, 469), (849, 470), (866, 480), (872, 480), (877, 479), (886, 469)]\n",
      "The chin in this face has the following points: [(1185, 496), (1185, 506), (1186, 516), (1188, 526), (1192, 536), (1199, 543), (1210, 549), (1221, 553), (1232, 554), (1242, 552), (1251, 546), (1259, 539), (1264, 530), (1267, 520), (1267, 509), (1266, 499), (1265, 489)]\n",
      "The left_eyebrow in this face has the following points: [(1192, 487), (1198, 481), (1206, 478), (1214, 479), (1222, 481)]\n",
      "The right_eyebrow in this face has the following points: [(1232, 481), (1239, 478), (1247, 477), (1254, 479), (1260, 483)]\n",
      "The nose_bridge in this face has the following points: [(1228, 489), (1228, 494), (1229, 500), (1230, 506)]\n",
      "The nose_tip in this face has the following points: [(1220, 513), (1225, 514), (1230, 514), (1234, 513), (1238, 512)]\n",
      "The left_eye in this face has the following points: [(1201, 491), (1206, 488), (1210, 488), (1215, 491), (1210, 491), (1205, 491)]\n",
      "The right_eye in this face has the following points: [(1238, 490), (1243, 487), (1247, 486), (1252, 489), (1248, 489), (1243, 490)]\n",
      "The top_lip in this face has the following points: [(1209, 525), (1216, 521), (1224, 518), (1230, 519), (1236, 518), (1244, 520), (1250, 523), (1247, 524), (1236, 522), (1230, 523), (1225, 523), (1211, 526)]\n",
      "The bottom_lip in this face has the following points: [(1250, 523), (1245, 532), (1237, 537), (1231, 537), (1225, 537), (1216, 534), (1209, 525), (1211, 526), (1225, 531), (1231, 531), (1236, 530), (1247, 524)]\n",
      "The chin in this face has the following points: [(676, 289), (676, 303), (677, 317), (679, 330), (683, 344), (693, 354), (705, 363), (716, 371), (729, 375), (740, 374), (750, 367), (758, 360), (765, 351), (770, 339), (773, 328), (776, 316), (778, 304)]\n",
      "The left_eyebrow in this face has the following points: [(695, 281), (703, 275), (714, 272), (725, 273), (734, 278)]\n",
      "The right_eyebrow in this face has the following points: [(746, 281), (755, 279), (765, 281), (772, 287), (775, 295)]\n",
      "The nose_bridge in this face has the following points: [(739, 292), (739, 301), (739, 309), (739, 318)]\n",
      "The nose_tip in this face has the following points: [(725, 323), (730, 325), (736, 328), (741, 327), (745, 326)]\n",
      "The left_eye in this face has the following points: [(706, 289), (712, 287), (718, 288), (724, 292), (717, 293), (711, 292)]\n",
      "The right_eye in this face has the following points: [(747, 297), (752, 294), (759, 295), (763, 299), (758, 300), (752, 299)]\n",
      "The top_lip in this face has the following points: [(711, 336), (720, 334), (728, 334), (734, 336), (739, 336), (745, 338), (750, 342), (747, 342), (739, 339), (733, 338), (727, 337), (714, 337)]\n",
      "The bottom_lip in this face has the following points: [(750, 342), (744, 348), (738, 350), (732, 350), (726, 348), (718, 344), (711, 336), (714, 337), (727, 344), (733, 345), (738, 345), (747, 342)]\n",
      "The chin in this face has the following points: [(1006, 394), (1006, 405), (1008, 417), (1010, 429), (1013, 440), (1020, 450), (1030, 458), (1042, 464), (1055, 465), (1068, 463), (1078, 457), (1087, 449), (1093, 439), (1096, 428), (1097, 416), (1098, 405), (1098, 394)]\n",
      "The left_eyebrow in this face has the following points: [(1017, 380), (1024, 375), (1033, 373), (1041, 375), (1049, 379)]\n",
      "The right_eyebrow in this face has the following points: [(1063, 378), (1071, 375), (1079, 373), (1087, 375), (1093, 382)]\n",
      "The nose_bridge in this face has the following points: [(1056, 388), (1056, 395), (1056, 401), (1056, 409)]\n",
      "The nose_tip in this face has the following points: [(1046, 415), (1051, 416), (1056, 418), (1060, 416), (1065, 415)]\n",
      "The left_eye in this face has the following points: [(1027, 390), (1032, 388), (1037, 388), (1042, 391), (1036, 392), (1031, 391)]\n",
      "The right_eye in this face has the following points: [(1069, 392), (1073, 389), (1079, 388), (1083, 390), (1079, 391), (1074, 392)]\n",
      "The top_lip in this face has the following points: [(1033, 431), (1041, 426), (1049, 424), (1056, 425), (1062, 424), (1070, 427), (1076, 431), (1074, 432), (1062, 429), (1055, 429), (1049, 429), (1035, 431)]\n",
      "The bottom_lip in this face has the following points: [(1076, 431), (1070, 441), (1062, 445), (1055, 446), (1048, 445), (1040, 440), (1033, 431), (1035, 431), (1049, 438), (1056, 439), (1062, 438), (1074, 432)]\n",
      "The chin in this face has the following points: [(1692, 436), (1692, 446), (1694, 456), (1696, 466), (1701, 475), (1707, 484), (1716, 491), (1726, 496), (1737, 496), (1749, 494), (1760, 488), (1771, 482), (1779, 472), (1783, 461), (1784, 449), (1784, 437), (1784, 424)]\n",
      "The left_eyebrow in this face has the following points: [(1694, 428), (1698, 422), (1705, 419), (1713, 419), (1720, 421)]\n",
      "The right_eyebrow in this face has the following points: [(1730, 419), (1738, 415), (1747, 412), (1757, 413), (1765, 417)]\n",
      "The nose_bridge in this face has the following points: [(1725, 428), (1726, 435), (1725, 442), (1726, 449)]\n",
      "The nose_tip in this face has the following points: [(1720, 455), (1724, 456), (1728, 457), (1733, 455), (1739, 453)]\n",
      "The left_eye in this face has the following points: [(1703, 433), (1707, 430), (1713, 429), (1719, 432), (1713, 433), (1708, 434)]\n",
      "The right_eye in this face has the following points: [(1739, 429), (1744, 424), (1750, 423), (1756, 425), (1751, 427), (1745, 428)]\n",
      "The top_lip in this face has the following points: [(1715, 468), (1720, 463), (1725, 462), (1731, 462), (1736, 460), (1745, 460), (1754, 462), (1752, 463), (1737, 464), (1731, 465), (1726, 465), (1718, 468)]\n",
      "The bottom_lip in this face has the following points: [(1754, 462), (1747, 471), (1739, 476), (1733, 477), (1727, 477), (1721, 474), (1715, 468), (1718, 468), (1726, 471), (1732, 471), (1738, 470), (1752, 463)]\n",
      "The chin in this face has the following points: [(1506, 485), (1504, 494), (1504, 503), (1504, 512), (1506, 522), (1511, 531), (1518, 539), (1527, 546), (1537, 549), (1549, 549), (1560, 545), (1570, 540), (1579, 532), (1584, 523), (1587, 513), (1589, 503), (1590, 493)]\n",
      "The left_eyebrow in this face has the following points: [(1515, 470), (1521, 467), (1527, 466), (1534, 467), (1540, 470)]\n",
      "The right_eyebrow in this face has the following points: [(1554, 472), (1561, 470), (1569, 470), (1576, 472), (1580, 477)]\n",
      "The nose_bridge in this face has the following points: [(1545, 481), (1544, 486), (1543, 491), (1542, 496)]\n",
      "The nose_tip in this face has the following points: [(1533, 504), (1537, 505), (1541, 506), (1545, 506), (1550, 506)]\n",
      "The left_eye in this face has the following points: [(1520, 482), (1525, 481), (1529, 481), (1534, 483), (1529, 484), (1525, 483)]\n",
      "The right_eye in this face has the following points: [(1557, 486), (1563, 485), (1567, 486), (1572, 487), (1567, 488), (1562, 487)]\n",
      "The top_lip in this face has the following points: [(1522, 517), (1529, 513), (1536, 512), (1540, 513), (1545, 513), (1552, 516), (1560, 522), (1557, 522), (1545, 517), (1540, 517), (1535, 516), (1525, 518)]\n",
      "The bottom_lip in this face has the following points: [(1560, 522), (1552, 530), (1544, 531), (1539, 531), (1534, 530), (1527, 526), (1522, 517), (1525, 518), (1534, 523), (1539, 524), (1544, 525), (1557, 522)]\n",
      "The chin in this face has the following points: [(484, 276), (488, 292), (491, 307), (496, 322), (502, 334), (513, 345), (527, 353), (543, 357), (560, 357), (574, 353), (586, 346), (596, 335), (602, 322), (604, 309), (604, 295), (604, 282), (603, 268)]\n",
      "The left_eyebrow in this face has the following points: [(501, 269), (509, 262), (519, 257), (532, 256), (543, 258)]\n",
      "The right_eyebrow in this face has the following points: [(557, 256), (567, 253), (577, 252), (587, 255), (593, 262)]\n",
      "The nose_bridge in this face has the following points: [(552, 265), (553, 275), (555, 285), (557, 296)]\n",
      "The nose_tip in this face has the following points: [(542, 300), (549, 302), (557, 304), (563, 301), (569, 298)]\n",
      "The left_eye in this face has the following points: [(515, 271), (521, 267), (528, 266), (535, 269), (529, 271), (522, 271)]\n",
      "The right_eye in this face has the following points: [(563, 268), (569, 263), (576, 263), (582, 267), (576, 268), (570, 268)]\n",
      "The top_lip in this face has the following points: [(529, 317), (539, 312), (549, 310), (557, 311), (564, 309), (572, 310), (580, 314), (577, 314), (564, 313), (557, 314), (550, 314), (532, 316)]\n",
      "The bottom_lip in this face has the following points: [(580, 314), (573, 320), (565, 323), (558, 324), (550, 324), (540, 322), (529, 317), (532, 316), (550, 320), (557, 320), (564, 319), (577, 314)]\n",
      "The chin in this face has the following points: [(1914, 411), (1916, 423), (1918, 434), (1921, 445), (1927, 456), (1935, 464), (1945, 470), (1957, 474), (1970, 474), (1983, 471), (1994, 463), (2005, 454), (2013, 443), (2017, 430), (2018, 417), (2016, 403), (2014, 390)]\n",
      "The left_eyebrow in this face has the following points: [(1916, 401), (1920, 394), (1927, 392), (1935, 392), (1944, 393)]\n",
      "The right_eyebrow in this face has the following points: [(1961, 390), (1970, 385), (1979, 382), (1990, 382), (1998, 386)]\n",
      "The nose_bridge in this face has the following points: [(1953, 400), (1954, 408), (1955, 415), (1956, 423)]\n",
      "The nose_tip in this face has the following points: [(1947, 429), (1953, 429), (1959, 430), (1965, 427), (1971, 424)]\n",
      "The left_eye in this face has the following points: [(1926, 407), (1931, 404), (1937, 403), (1943, 404), (1937, 405), (1932, 406)]\n",
      "The right_eye in this face has the following points: [(1970, 399), (1975, 395), (1980, 394), (1986, 395), (1981, 396), (1976, 398)]\n",
      "The top_lip in this face has the following points: [(1941, 443), (1947, 437), (1954, 435), (1961, 435), (1968, 432), (1977, 431), (1988, 432), (1985, 434), (1969, 436), (1962, 438), (1955, 439), (1944, 443)]\n",
      "The bottom_lip in this face has the following points: [(1988, 432), (1980, 443), (1972, 450), (1964, 452), (1957, 453), (1949, 450), (1941, 443), (1944, 443), (1956, 446), (1963, 445), (1970, 443), (1985, 434)]\n",
      "The chin in this face has the following points: [(225, 370), (226, 387), (228, 404), (232, 421), (240, 436), (254, 448), (269, 457), (286, 465), (303, 467), (318, 465), (329, 454), (338, 443), (346, 431), (351, 417), (353, 404), (352, 390), (349, 377)]\n",
      "The left_eyebrow in this face has the following points: [(252, 353), (262, 345), (274, 342), (287, 346), (299, 351)]\n",
      "The right_eyebrow in this face has the following points: [(314, 352), (323, 349), (333, 348), (342, 351), (345, 361)]\n",
      "The nose_bridge in this face has the following points: [(307, 365), (308, 375), (310, 384), (312, 393)]\n",
      "The nose_tip in this face has the following points: [(292, 400), (300, 402), (309, 405), (315, 404), (321, 402)]\n",
      "The left_eye in this face has the following points: [(268, 363), (275, 361), (282, 361), (288, 366), (281, 365), (274, 364)]\n",
      "The right_eye in this face has the following points: [(317, 369), (324, 366), (331, 366), (336, 370), (330, 370), (324, 370)]\n",
      "The top_lip in this face has the following points: [(272, 416), (285, 412), (299, 411), (307, 413), (315, 412), (324, 414), (331, 419), (327, 419), (315, 417), (307, 417), (298, 416), (276, 417)]\n",
      "The bottom_lip in this face has the following points: [(331, 419), (323, 431), (314, 434), (306, 435), (296, 434), (284, 428), (272, 416), (276, 417), (297, 426), (306, 427), (314, 427), (327, 419)]\n",
      "The chin in this face has the following points: [(2073, 282), (2075, 294), (2078, 306), (2081, 318), (2087, 329), (2096, 338), (2106, 345), (2118, 351), (2131, 351), (2144, 348), (2156, 339), (2167, 329), (2175, 317), (2179, 303), (2180, 288), (2179, 274), (2179, 260)]\n",
      "The left_eyebrow in this face has the following points: [(2076, 275), (2080, 267), (2089, 264), (2099, 263), (2109, 265)]\n",
      "The right_eyebrow in this face has the following points: [(2123, 260), (2133, 256), (2143, 253), (2154, 253), (2163, 259)]\n",
      "The nose_bridge in this face has the following points: [(2117, 272), (2118, 283), (2119, 293), (2120, 304)]\n",
      "The nose_tip in this face has the following points: [(2111, 307), (2116, 309), (2122, 310), (2128, 307), (2133, 303)]\n",
      "The left_eye in this face has the following points: [(2088, 279), (2093, 276), (2100, 275), (2106, 278), (2100, 279), (2094, 280)]\n",
      "The right_eye in this face has the following points: [(2133, 272), (2139, 268), (2145, 266), (2152, 267), (2147, 270), (2140, 272)]\n",
      "The top_lip in this face has the following points: [(2103, 316), (2112, 318), (2118, 317), (2124, 317), (2129, 315), (2137, 313), (2148, 309), (2145, 311), (2130, 318), (2124, 319), (2119, 319), (2106, 317)]\n",
      "The bottom_lip in this face has the following points: [(2148, 309), (2139, 318), (2131, 323), (2125, 324), (2120, 325), (2112, 323), (2103, 316), (2106, 317), (2119, 319), (2124, 319), (2130, 317), (2145, 311)]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import face_recognition\n",
    "\n",
    "# Load the jpg file into a numpy array\n",
    "image = face_recognition.load_image_file(\"far_portrait_multiple.png\") #INSERT IMAGE HERE\n",
    "\n",
    "# Find all facial features in all the faces in the image\n",
    "face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "\n",
    "print(\"I found {} face(s) in this photograph.\".format(len(face_landmarks_list)))\n",
    "\n",
    "# Create a PIL imagedraw object so we can draw on the picture\n",
    "pil_image = Image.fromarray(image)\n",
    "d = ImageDraw.Draw(pil_image)\n",
    "\n",
    "for face_landmarks in face_landmarks_list:\n",
    "\n",
    "    # Print the location of each facial feature in this image\n",
    "    for facial_feature in face_landmarks.keys():\n",
    "        print(\"The {} in this face has the following points: {}\".format(facial_feature, face_landmarks[facial_feature]))\n",
    "\n",
    "    # Let's trace out each facial feature in the image with a line!\n",
    "    for facial_feature in face_landmarks.keys():\n",
    "        d.line(face_landmarks[facial_feature], width=5)\n",
    "\n",
    "# Show the picture\n",
    "pil_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can also do really stupid stuff with applying digital make-up.  This could be the result: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"example_2_1.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: We are using `biden.jpg` for this example. <br>\n",
    "> **Note:** We are importing `face_recognition` and `Image` and `ImageDraw` from `PIL` here.\n",
    "> **Tip:** The following key functions are used in this sample code. \n",
    "- `polygon`: Draws a polygon.\n",
    "- `line`: Draws a line between the coordinates in the `xy` list.\n",
    "\n",
    "> Reference: Information about `ImageDraw` module. https://pillow.readthedocs.io/en/stable/reference/ImageDraw.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import face_recognition\n",
    "\n",
    "# Load the jpg file into a numpy array\n",
    "image = face_recognition.load_image_file(\"biden.jpg\")\n",
    "\n",
    "# Find all facial features in all the faces in the image\n",
    "face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "\n",
    "pil_image = Image.fromarray(image)\n",
    "for face_landmarks in face_landmarks_list:\n",
    "    d = ImageDraw.Draw(pil_image, 'RGBA')\n",
    "\n",
    "    # Make the eyebrows into a nightmare\n",
    "    d.polygon(face_landmarks['left_eyebrow'], fill=(68, 54, 39, 128))\n",
    "    d.polygon(face_landmarks['right_eyebrow'], fill=(68, 54, 39, 128))\n",
    "    d.line(face_landmarks['left_eyebrow'], fill=(68, 54, 39, 150), width=5)\n",
    "    d.line(face_landmarks['right_eyebrow'], fill=(68, 54, 39, 150), width=5)\n",
    "\n",
    "    # Gloss the lips\n",
    "    d.polygon(face_landmarks['top_lip'], fill=(150, 0, 0, 128))\n",
    "    d.polygon(face_landmarks['bottom_lip'], fill=(150, 0, 0, 128))\n",
    "    d.line(face_landmarks['top_lip'], fill=(150, 0, 0, 64), width=8)\n",
    "    d.line(face_landmarks['bottom_lip'], fill=(150, 0, 0, 64), width=8)\n",
    "\n",
    "    # Sparkle the eyes\n",
    "    d.polygon(face_landmarks['left_eye'], fill=(255, 255, 255, 30))\n",
    "    d.polygon(face_landmarks['right_eye'], fill=(255, 255, 255, 30))\n",
    "\n",
    "    # Apply some eyeliner\n",
    "    d.line(face_landmarks['left_eye'] + [face_landmarks['left_eye'][0]], fill=(0, 0, 0, 110), width=6)\n",
    "    d.line(face_landmarks['right_eye'] + [face_landmarks['right_eye'][0]], fill=(0, 0, 0, 110), width=6)\n",
    "\n",
    "    pil_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part_1.3'></a>\n",
    "### 1.3 Recognize faces in images and identify who they are (`compare_faces`)\n",
    "[Back to the table of contents.](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recognize who appears in each photo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"example_3.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We are trying to classify `obama2.jpg`, which we will define as our \"unknown\" image.  The function `face_encodings` should classify this image as Barack Obama. To find this out, \n",
    "- we load the JPG/PNG files into numpy arrays, \n",
    "- get the face encodings for each face in each image file, \n",
    "- and outputs the results.\n",
    "<br>\n",
    "\n",
    "> (Note: The `tolerance` is set at 0.6 because it is normally displays the best results.  Change if needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the unknown face a picture of Biden? False\n",
      "Is the unknown face a picture of Obama? True\n",
      "Is the unknown face a new person that we've never seen before? False\n"
     ]
    }
   ],
   "source": [
    "# Load the jpg files into numpy arrays\n",
    "biden_image = face_recognition.load_image_file(\"biden.jpg\")\n",
    "obama_image = face_recognition.load_image_file(\"obama.jpg\")\n",
    "unknown_image = face_recognition.load_image_file(\"obama2.jpg\") # UNKKNOWN IMAGE\n",
    "\n",
    "# Get the face encodings for each face in each image file\n",
    "# Since there could be more than one face in each image, it returns a list of encodings.\n",
    "# But since I know each image only has one face, I only care about the first encoding in each image, so I grab index 0.\n",
    "try:\n",
    "    biden_face_encoding = face_recognition.face_encodings(biden_image)[0]\n",
    "    obama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n",
    "    unknown_face_encoding = face_recognition.face_encodings(unknown_image)[0]\n",
    "except IndexError:\n",
    "    print(\"I wasn't able to locate any faces in at least one of the images. Check the image files. Aborting...\")\n",
    "    quit()\n",
    "\n",
    "known_faces = [\n",
    "    biden_face_encoding,\n",
    "    obama_face_encoding\n",
    "]\n",
    "\n",
    "# results is an array of True/False telling if the unknown face matched anyone in the known_faces array\n",
    "results = face_recognition.compare_faces(known_faces, unknown_face_encoding)\n",
    "\n",
    "print(\"Is the unknown face a picture of Biden? {}\".format(results[0]))\n",
    "print(\"Is the unknown face a picture of Obama? {}\".format(results[1]))\n",
    "print(\"Is the unknown face a new person that we've never seen before? {}\".format(not True in results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part_1.4'></a>\n",
    "### 1.4 Face recognition from webcam (`video_capture`)\n",
    "[Back to the table of contents.](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can even use this library with other Python libraries to do real-time face recognition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"example_4.gif\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Instructions:** Run the code cell below.  After doing so, a screen should pop up showing your webcam.  If you define your face encoding in `face_encodings`, the program should work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The `face_recognition`, `cv2`, and `numpy` libraries are imported. <br>\n",
    "> Note: Click \"Python\" > \"Hide\" to exit window.  However, the video will continue running.  So, interrupt the cell to stop the code from running.  (But the video camera will still be on.)\n",
    "- Instead, click \"Python\" > \"Exit\" to quit.  However, doing so will stop Kernel, so make sure to restart Kernel. However, the video camera will turn off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing appropriate libraries\n",
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# This is a demo of running face recognition on live video from your webcam. It's a little more complicated than the\n",
    "# other example, but it includes some basic performance tweaks to make things run a lot faster:\n",
    "#   1. Process each video frame at 1/4 resolution (though still display it at full resolution)\n",
    "#   2. Only detect faces in every other frame of video.\n",
    "\n",
    "# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n",
    "# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n",
    "# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n",
    "\n",
    "# Get a reference to webcam #0 (the default one)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Load a sample picture and learn how to recognize it.\n",
    "obama_image = face_recognition.load_image_file(\"obama.jpg\")\n",
    "obama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n",
    "\n",
    "# Load a second sample picture and learn how to recognize it.\n",
    "biden_image = face_recognition.load_image_file(\"biden.jpg\")\n",
    "biden_face_encoding = face_recognition.face_encodings(biden_image)[0]\n",
    "\n",
    "# ADDED: Joshua's sample picture.\n",
    "joshua_image = face_recognition.load_image_file(\"close_portrait.png\")\n",
    "joshua_face_encoding = face_recognition.face_encodings(joshua_image)[0]\n",
    "\n",
    "# Create arrays of known face encodings and their names\n",
    "known_face_encodings = [\n",
    "    obama_face_encoding,\n",
    "    biden_face_encoding,\n",
    "    joshua_face_encoding # ADDED\n",
    "]\n",
    "known_face_names = [\n",
    "    \"Barack Obama\",\n",
    "    \"Joe Biden\"\n",
    "    \"Joshua Sanchez\" # ADDED\n",
    "]\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "    # Only process every other frame of video to save time\n",
    "    if process_this_frame:\n",
    "        # Find all the faces and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # See if the face is a match for the known face(s)\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            name = \"Unknown\"\n",
    "\n",
    "            # # If a match was found in known_face_encodings, just use the first one.\n",
    "            # if True in matches:\n",
    "            #     first_match_index = matches.index(True)\n",
    "            #     name = known_face_names[first_match_index]\n",
    "\n",
    "            # Or instead, use the known face with the smallest distance to the new face\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "            face_names.append(name)\n",
    "\n",
    "    process_this_frame = not process_this_frame\n",
    "\n",
    "\n",
    "    # Display the results\n",
    "    for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "        top *= 4\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part_2'></a>\n",
    "## 2. EmoPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part_2.0'></a>\n",
    "### 2.0 Introduction & Setup\n",
    "[Back to the table of contents.](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction- About the library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> EmoPy is a python toolkit with deep neural net classes which aims to make accurate predictions of emotions given images of people's faces.\n",
    "\n",
    "> EmoPy includes several modules that are plugged together to build a trained [Facial Expression Recognition (FER)](https://en.wikipedia.org/wiki/Emotion_recognition) prediction model.\n",
    "- `fermodel.py`\n",
    "- `euralnets.py`\n",
    "- `dataset.py`\n",
    "- `data_loader.py`\n",
    "- `csv_data_loader.py`\n",
    "- `directory_data_loader.py`\n",
    "- `data_generator.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction- About the Files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following images are used in the notebook (scaled down below). You can access them directly:\n",
    "- `anger_image.png`: Photo of angry face.\n",
    "- `disgust_image.png`: Photo of disgust face.\n",
    "- `happy_image.png`: Photo of happy face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"anger_image.png\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "<td> <img src=\"disgust_image.png\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "<td> <img src=\"happy_image.png\" alt=\"Drawing\" style=\"width: 200px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup- Installing libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First, check your Python version.  EmoPy uses only Python versions 3.6 and up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8\n"
     ]
    }
   ],
   "source": [
    "# Please check if you have Python 3.6 and up installed:\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then, make sure that you are able to download EmoPy.  This might also mean downloading Tensorflow: \n",
    "- `wrapt`: A Python module for decorators, wrappers and monkey patching.\n",
    "- `Tensorflow`: TensorFlow is a free and open-source software library for dataflow and differentiable programming across a range of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/Joshua/Library/Caches/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63/wrapt-1.12.1-cp36-cp36m-macosx_10_7_x86_64.whl\n",
      "Installing collected packages: wrapt\n",
      "Successfully installed wrapt-1.12.1\n",
      "Requirement already up-to-date: tensorflow==2.0.0-beta1 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (2.0.0b1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (0.31.1)\n",
      "Requirement already satisfied, skipping upgrade: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (1.14.0.dev2019060501)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (1.0.6)\n",
      "Requirement already satisfied, skipping upgrade: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (1.14.0a20190603)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (1.14.5)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (3.12.2)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta1) (1.16.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.7.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (49.2.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.0.1)\n",
      "Requirement already satisfied: EmoPy in /Users/Joshua/anaconda3/lib/python3.6/site-packages (0.0.5)\n",
      "Requirement already satisfied: scikit-neuralnetwork>=0.7 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (0.7)\n",
      "Requirement already satisfied: scikit-image>=0.13.1 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (0.13.1)\n",
      "Requirement already satisfied: pytest in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (3.5.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (1.1.0)\n",
      "Requirement already satisfied: lasagne in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (0.1)\n",
      "Requirement already satisfied: numpy<=1.14.5,>=1.13.3 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (1.14.5)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (0.21.3)\n",
      "Requirement already satisfied: opencv-python in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (4.3.0.36)\n",
      "Requirement already satisfied: keras>=2.2.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (2.3.1)\n",
      "Requirement already satisfied: matplotlib>2.1.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (3.1.1)\n",
      "Requirement already satisfied: h5py in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (2.7.1)\n",
      "Requirement already satisfied: tensorflow>=1.10.1 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (2.0.0b1)\n",
      "Requirement already satisfied: pydot in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (1.4.1)\n",
      "Requirement already satisfied: graphviz in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from EmoPy) (0.14.1)\n",
      "Requirement already satisfied: Theano>=0.8 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from scikit-neuralnetwork>=0.7->EmoPy) (1.0.4)\n",
      "Requirement already satisfied: six>=1.7.3 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from scikit-image>=0.13.1->EmoPy) (1.11.0)\n",
      "Requirement already satisfied: networkx>=1.8 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from scikit-image>=0.13.1->EmoPy) (2.1)\n",
      "Requirement already satisfied: pillow>=2.1.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from scikit-image>=0.13.1->EmoPy) (5.0.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from scikit-image>=0.13.1->EmoPy) (0.5.2)\n",
      "Requirement already satisfied: py>=1.5.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from pytest->EmoPy) (1.5.3)\n",
      "Requirement already satisfied: setuptools in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from pytest->EmoPy) (49.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from pytest->EmoPy) (18.1.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from pytest->EmoPy) (4.1.0)\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from pytest->EmoPy) (0.6.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from scikit-learn>=0.19.1->EmoPy) (0.13.2)\n",
      "Requirement already satisfied: pyyaml in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from keras>=2.2.0->EmoPy) (3.12)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from keras>=2.2.0->EmoPy) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from keras>=2.2.0->EmoPy) (1.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from matplotlib>2.1.0->EmoPy) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from matplotlib>2.1.0->EmoPy) (2.7.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from matplotlib>2.1.0->EmoPy) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from matplotlib>2.1.0->EmoPy) (2.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow>=1.10.1->EmoPy) (1.12.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow>=1.10.1->EmoPy) (1.16.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow>=1.10.1->EmoPy) (1.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow>=1.10.1->EmoPy) (1.14.0a20190603)\n",
      "Requirement already satisfied: gast>=0.2.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow>=1.10.1->EmoPy) (0.3.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow>=1.10.1->EmoPy) (0.9.0)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow>=1.10.1->EmoPy) (1.14.0.dev2019060501)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow>=1.10.1->EmoPy) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow>=1.10.1->EmoPy) (0.31.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow>=1.10.1->EmoPy) (3.12.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tensorflow>=1.10.1->EmoPy) (0.7.1)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from networkx>=1.8->scikit-image>=0.13.1->EmoPy) (4.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow>=1.10.1->EmoPy) (3.0.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/Joshua/anaconda3/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow>=1.10.1->EmoPy) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "# Below is the code we used.\n",
    "!pip3 install wrapt --upgrade --ignore-installed\n",
    "\n",
    "# This may take a couple minutes.\n",
    "!pip install --upgrade tensorflow==2.0.0-beta1\n",
    "\n",
    "# This may take a couple minutes.\n",
    "!pip install EmoPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part_2.1'></a>\n",
    "### 2.1 Generate emotion probabilites for sample image (`predict`)\n",
    "[Back to the table of contents.](#table_of_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Import `FERModel` from `EmoPy`. \n",
    "> Note that the images are taken from the `Emopy.examples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Joshua/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing FER model parameters for target emotions: ['calm', 'anger', 'happiness']\n",
      "WARNING:tensorflow:From /Users/Joshua/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Predicting on happy image...\n",
      "anger: 44.1%\n",
      "calm: 9.5%\n",
      "happiness: 46.4%\n",
      "Dominant emotion: happiness\n",
      "\n",
      "Predicting on disgust image...\n",
      "anger: 29.9%\n",
      "calm: 16.0%\n",
      "happiness: 54.1%\n",
      "Dominant emotion: happiness\n",
      "\n",
      "Predicting on anger image...\n",
      "anger: 63.1%\n",
      "calm: 22.9%\n",
      "happiness: 14.0%\n",
      "Dominant emotion: anger\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from EmoPy.src.fermodel import FERModel\n",
    "from pkg_resources import resource_filename\n",
    "\n",
    "target_emotions = ['calm', 'anger', 'happiness']\n",
    "model = FERModel(target_emotions, verbose=True)\n",
    "\n",
    "print('Predicting on happy image...')\n",
    "model.predict(resource_filename('EmoPy.examples','image_data/sample_happy_image.png'))\n",
    "\n",
    "print('Predicting on disgust image...')\n",
    "model.predict(resource_filename('EmoPy.examples','image_data/sample_disgust_image.png'))\n",
    "\n",
    "print('Predicting on anger image...')\n",
    "model.predict(resource_filename('EmoPy.examples','image_data/sample_anger_image2.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of content.\n",
    "[Back to the table of contents.](#table_of_contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
